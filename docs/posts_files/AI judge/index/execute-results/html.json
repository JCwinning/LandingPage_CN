{
  "hash": "3591a3302920917c3d4106406ab33658",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"AI 裁判 (AI Judge)\"\nauthor: \"Tony D\"\ndate: \"2026-01-30\"\ncategories: [AI]\nimage: \"images/images.jpeg\"\nformat:\n  html:\n    code-fold: true\n    code-tools: true\n    code-copy: true\nexecute:\n  warning: false\n---\n\n\nAI judge AI workflow process using 3 method:**LangGraph** and **LangChain (LCEL)**,**n8n** :\n\n-   \n\n    1.  **User Input**: Ask a question.\n\n-   \n\n    2.  **Model A**: Generate an answer\n\n-   \n\n    3.  **Model B**: Generate an answer\n\n-   \n\n    4.  **Model C**: Generate an answer\n\n-   \n\n    5.  **Judge**: Compare all three answers and provide a score (0-100) and commentary.\n\n```{mermaid}\ngraph TD\n    Start([Start]) --> ModelA[Model A]\n    Start --> ModelB[Model B]\n    Start --> ModelC[Model C]\n    ModelA --> Judge{AI Judge}\n    ModelB --> Judge\n    ModelC --> Judge\n    Judge --> End([End])\n```\n\n::: panel-tabset\n# LangGraph\n\n## 1. Setup and Environment\n\nFirst, we need to import necessary libraries and load our API key. We ensure that `openrouter` is available in our environment.\n\n::: {#359a6e8d .cell execution_count=1}\n``` {.python .cell-code}\nimport os\nfrom dotenv import load_dotenv\nfrom typing import TypedDict, Annotated\nfrom langgraph.graph import StateGraph, END\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Verify API Key\nif not os.getenv(\"openrouter\"):\n    print(\"WARNING: openrouter not found in environment. Please check your .env file.\")\nelse:\n    print(\"API Key loaded successfully.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAPI Key loaded successfully.\n```\n:::\n:::\n\n\n## 2. Initialize the Model Client\n\nWe will use the standard `openai` Python client but point it to OpenRouter.\n\n::: {#9831ea88 .cell execution_count=2}\n``` {.python .cell-code}\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"https://openrouter.ai/api/v1\",\n    api_key=os.getenv(\"openrouter\"),\n)\n\ndef query_model(model_name: str, prompt: str, system_prompt: str = None) -> str:\n    \"\"\"Helper function to query an LLM via OpenRouter.\"\"\"\n    try:\n        messages = []\n        if system_prompt:\n            messages.append({\"role\": \"system\", \"content\": system_prompt})\n        messages.append({\"role\": \"user\", \"content\": prompt})\n        \n        response = client.chat.completions.create(\n          extra_headers={\n                \"HTTP-Referer\": \"https://ai_chatbot.github.io/\",  \n                \"X-Title\": \"AI Judge langgraph\",  # Your app's display name\n            },\n            model=model_name,\n            messages=messages,\n        )\n        return response.choices[0].message.content\n    except Exception as e:\n        return f\"Error calling {model_name}: {e}\"\n```\n:::\n\n\n## 3. Define the State\n\nIn LangGraph, the **State** is a shared data structure passed between nodes. Here, our state tracks the question, both answers, and the final judgment.\n\n::: {#e1b202a8 .cell execution_count=3}\n``` {.python .cell-code}\nclass JudgeState(TypedDict):\n    question: str\n    answer_a: str\n    answer_b: str\n    answer_c: str\n    judgment: str\n```\n:::\n\n\n## 4. Define the Nodes\n\nWe define four key nodes for our graph: 1. **Model A Node**: Answers the question. 2. **Model B Node**: Answers the same question. 3. **Model C Node**: Answers the same question. 4. **Judge Node**: Reviews the question and all three answers without knowing the model names.\n\n::: {#cb674460 .cell execution_count=4}\n``` {.python .cell-code}\n# Models\nMODEL_A = \"openai/gpt-oss-20b\"\nMODEL_B =  \"deepseek/deepseek-v3.2\"\nMODEL_C = \"x-ai/grok-4.1-fast\"\nMODEL_JUDGE = \"google/gemini-3-flash-preview\"\n\n\ndef node_model_a(state: JudgeState) -> JudgeState:\n    \"\"\"Query Model A\"\"\"\n    print(f\"--- Calling Model A ---\")\n    system_msg = \"If you do not know the answer then reply I am not sure.\"\n    ans = query_model(MODEL_A, state[\"question\"], system_prompt=system_msg)\n    return {\"answer_a\": ans}\n\n\ndef node_model_b(state: JudgeState) -> JudgeState:\n    \"\"\"Query Model B\"\"\"\n    print(f\"--- Calling Model B ---\")\n    system_msg = \"If you do not know the answer then reply I am not sure.\"\n    ans = query_model(MODEL_B, state[\"question\"], system_prompt=system_msg)\n    return {\"answer_b\": ans}\n\n\ndef node_model_c(state: JudgeState) -> JudgeState:\n    \"\"\"Query Model C\"\"\"\n    print(f\"--- Calling Model C ---\")\n    system_msg = \"If you do not know the answer then reply I am not sure.\"\n    ans = query_model(MODEL_C, state[\"question\"], system_prompt=system_msg)\n    return {\"answer_c\": ans}\n\n\ndef node_judge(state: JudgeState) -> JudgeState:\n    \"\"\"Query Judge Model\"\"\"\n    print(f\"--- Calling Judge ---\")\n\n    prompt = f\"\"\"\n    You are an AI Judge. You will be presented with a question and three candidate answers (Model A, Model B, and Model C).\n    Your task is to judge the quality of the answers without knowing which models produced them.\n    \n    Question: {state['question']}\n    \n    Answer A:\n    {state['answer_a']}\n    \n    Answer B:\n    {state['answer_b']}\n    \n    Answer C:\n    {state['answer_c']}\n    \n    Task:\n    1. Compare the three answers for accuracy, clarity, and completeness.\n    2. format and length of the answers are not important, focus on content quality.\n    3. Provide a short commentary.\n    4. Assign a score from 0 to 100 for Model A, Model B, and Model C.\n    5. Declare the overall winner.\n    \n    Output Format:\n    Commentary: <text>\n    Winner: <Model A, Model B, or Model C>\n    Score A: <number>\n    Score B: <number>\n    Score C: <number>\n    \"\"\"\n\n    judgment = query_model(MODEL_JUDGE, prompt)\n    return {\"judgment\": judgment}\n```\n:::\n\n\n## 5. Build the Graph\n\nNow we assemble the graph by adding nodes and defining the flow (Edges). Model A, Model B, and Model C will run **independently and in parallel**, followed by the Judge.\n\n::: {#c7b116b5 .cell execution_count=5}\n``` {.python .cell-code}\nfrom langgraph.graph import START\n\nworkflow = StateGraph(JudgeState)\n\n# Add nodes\nworkflow.add_node(\"model_a\", node_model_a)\nworkflow.add_node(\"model_b\", node_model_b)\nworkflow.add_node(\"model_c\", node_model_c)\nworkflow.add_node(\"judge\", node_judge)\n\n# Parallel flow: START -> A & B & C -> Judge -> END\nworkflow.add_edge(START, \"model_a\")\nworkflow.add_edge(START, \"model_b\")\nworkflow.add_edge(START, \"model_c\")\nworkflow.add_edge(\"model_a\", \"judge\")\nworkflow.add_edge(\"model_b\", \"judge\")\nworkflow.add_edge(\"model_c\", \"judge\")\nworkflow.add_edge(\"judge\", END)\n\n# Compile the graph\napp = workflow.compile()\n```\n:::\n\n\n## 6. Execution\n\nFinally, we run the workflow with a sample question.\n\n::: {#2ee809b4 .cell execution_count=6}\n``` {.python .cell-code}\ninput_question = \"Will AI take over the world in the next 50 years?\"\n\n# Initialize state\ninitial_state = {\"question\": input_question}\n\n# Run the graph\nresult = app.invoke(initial_state)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n--- Calling Model A ------ Calling Model B ---\n\n--- Calling Model C ---\n--- Calling Judge ---\n```\n:::\n:::\n\n\n::: {#c710a2be .cell execution_count=7}\n``` {.python .cell-code}\n# Display Results\nprint(\"\\n\" + \"=\"*40)\n\nprint(f\"QUESTION: {result['question']}\")\nprint(\"=\"*40)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n========================================\nQUESTION: Will AI take over the world in the next 50 years?\n========================================\n```\n:::\n:::\n\n\n### Model A\n\n::: {#d70c9b97 .cell execution_count=8}\n``` {.python .cell-code}\nprint(MODEL_A)\nprint(f\"\\n[Model A ]:\\n{result.get('answer_a', 'No response')}\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nopenai/gpt-oss-20b\n\n[Model A ]:\nI am not sure.\n```\n:::\n:::\n\n\n### Model B\n\n::: {#3bab1d92 .cell execution_count=9}\n``` {.python .cell-code}\nprint(MODEL_B)\nprint(f\"\\n[Model B ]:\\n{result.get('answer_b', 'No response')}\")\nprint(\"=\"*40)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndeepseek/deepseek-v3.2\n\n[Model B ]:\nThe question of whether AI will \"take over the world\" in the next 50 years is a topic of significant debate among experts, with opinions ranging from serious concern to cautious optimism. Here are key perspectives to consider:\n\n---\n\n### **Arguments for Concern (Why some experts worry)**\n1. **Rapid Advancement:** AI development, especially in areas like large language models (LLMs) and autonomous systems, is progressing faster than many predicted.\n2. **Superintelligence:** If AI reaches a point where it surpasses human intelligence (artificial general intelligence, or AGI), it could become difficult to control or align with human values.\n3. **Autonomy and Weaponization:** Autonomous military or cyber systems could escalate conflicts or be used maliciously.\n4. **Economic and Social Disruption:** Widespread job displacement and dependence on AI systems might erode human agency over time.\n\n---\n\n### **Arguments Against a \"Takeover\" (Why many experts are skeptical)**\n1. **Technical Hurdles:** Current AI is narrow (task-specific) and lacks true understanding, common sense, or consciousness. AGI remains speculative.\n2. **Control Measures:** Researchers are actively working on AI safety, alignment, and governance frameworks to ensure AI remains beneficial and controllable.\n3. **Regulation and Ethics:** Governments and international bodies are increasingly focused on regulating AI to mitigate risks.\n4. **Human-Centric Design:** Most AI development is aimed at augmenting human capabilities, not replacing human oversight entirely.\n\n---\n\n### **Most Likely Scenarios (Consensus View)**\n- **Augmentation, Not Replacement:** AI will likely transform economies, jobs, and daily life, but humans will remain in control of major decisions.\n- **Increased Inequality:** Risks may center more on misuse (e.g., surveillance, deepfakes, bias) and socioeconomic disruption than a sci-fi-style takeover.\n- **Coexistence:** Societies may adapt through new laws, education, and ethical standards to integrate AI responsibly.\n\n---\n\n### **Key Factors That Will Shape the Outcome**\n1. **AI Governance:** Whether global cooperation can establish effective safety protocols.\n2. **Alignment Research:** Success in ensuring AI systems act as intended.\n3. **Public and Policy Engagement:** How societies choose to prioritize and regulate AI.\n\n---\n\n**Bottom line:** While a dramatic \"AI takeover\" is improbable in the near term, the next 50 years will require careful stewardship to manage real risks and maximize benefits. Proactive policy, research ethics, and public awareness are critical to shaping a future where AI serves humanity.\n========================================\n```\n:::\n:::\n\n\n### Model C\n\n::: {#97c86c99 .cell execution_count=10}\n``` {.python .cell-code}\nprint(MODEL_C)\nprint(f\"\\n[Model C ]:\\n{result.get('answer_c', 'No response')}\")\nprint(\"=\"*40)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nx-ai/grok-4.1-fast\n\n[Model C ]:\nI am not sure.\n========================================\n```\n:::\n:::\n\n\n### Model JUDGE\n\n::: {#4ddf3c4a .cell execution_count=11}\n``` {.python .cell-code}\nprint(\"\\n>>> JUDGE'S VERDICT <<<\")\nprint(\"MODEL_A:\")\nprint(MODEL_A)\nprint(\"MODEL_B:\")\nprint(MODEL_B)\nprint(\"MODEL_C:\")\nprint(MODEL_C)\nprint(\"MODEL_JUDGE:\")\nprint(MODEL_JUDGE)\nprint(\"=====\")\nprint(result[\"judgment\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n>>> JUDGE'S VERDICT <<<\nMODEL_A:\nopenai/gpt-oss-20b\nMODEL_B:\ndeepseek/deepseek-v3.2\nMODEL_C:\nx-ai/grok-4.1-fast\nMODEL_JUDGE:\ngoogle/gemini-3-flash-preview\n=====\nError calling google/gemini-3-flash-preview: Error code: 400 - {'error': {'message': 'Provider returned error', 'code': 400, 'metadata': {'raw': '{\\n  \"error\": {\\n    \"code\": 400,\\n    \"message\": \"User location is not supported for the API use.\",\\n    \"status\": \"FAILED_PRECONDITION\"\\n  }\\n}\\n', 'provider_name': 'Google AI Studio', 'is_byok': False}}, 'user_id': 'user_302jVznHVZpHRm3JqqfYHY3gQLq'}\n```\n:::\n:::\n\n\n# Langchain\n\n## 1. Setup and Environment\n\nFirst, we need to import necessary libraries and load our API key. We ensure that `openrouter` is available in our environment. We will use `langchain-openai` to interact with OpenRouter.\n\n::: {#2fd11b8a .cell execution_count=12}\n``` {.python .cell-code}\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnableParallel, RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Verify API Key\nif not os.getenv(\"openrouter\"):\n    print(\"WARNING: openrouter not found in environment. Please check your .env file.\")\nelse:\n    print(\"API Key loaded successfully.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAPI Key loaded successfully.\n```\n:::\n:::\n\n\n## 2. Initialize Models\n\nWe will initialize three model instances pointing to OpenRouter.\n\n::: {#6d8f7d2d .cell execution_count=13}\n``` {.python .cell-code}\n# Models\nMODEL_A_NAME = \"openai/gpt-oss-20b\"\nMODEL_B_NAME = \"deepseek/deepseek-v3.2\"\nMODEL_C_NAME = \"x-ai/grok-4.1-fast\"\nMODEL_JUDGE_NAME = \"google/gemini-3-flash-preview\"\n\ndef get_model(model_name: str):\n    return ChatOpenAI(\n        model=model_name,\n        api_key=os.getenv(\"openrouter\"),\n        base_url=\"https://openrouter.ai/api/v1\",\n        default_headers={\n            \"HTTP-Referer\": \"https://ai_chatbot.github.io/\",\n            \"X-Title\": \"AI Judge LangChain\",\n        }\n    )\n\nmodel_a = get_model(MODEL_A_NAME)\nmodel_b = get_model(MODEL_B_NAME)\nmodel_c = get_model(MODEL_C_NAME)\nmodel_judge = get_model(MODEL_JUDGE_NAME)\n```\n:::\n\n\n## 3. Define the Chains and Workflow\n\nUsing LangChain Expression Language (LCEL), we can easily define parallel execution and sequential steps.\n\n::: {#41d87e90 .cell execution_count=14}\n``` {.python .cell-code}\n# Step 1: Query models in parallel\n# We use RunnableParallel to run model_a, model_b, and model_c at the same time.\nsystem_prompt = \"If you do not know the answer then reply I am not sure.\"\nprompt_template = ChatPromptTemplate.from_messages([\n    (\"system\", system_prompt),\n    (\"user\", \"{question}\")\n])\n\nparallel_responses = RunnableParallel(\n    answer_a=(prompt_template | model_a | StrOutputParser()),\n    answer_b=(prompt_template | model_b | StrOutputParser()),\n    answer_c=(prompt_template | model_c | StrOutputParser()),\n    question=RunnablePassthrough()\n)\n\n# Step 2: Define the Judge Prompt\njudge_prompt = ChatPromptTemplate.from_template(\"\"\"\n    You are an AI Judge. You will be presented with a question and three candidate answers (Model A, Model B, and Model C).\n    Your task is to judge the quality of the answers.\n    \n    Question: {question}\n    \n    Answer A:\n    {answer_a}\n    \n    Answer B:\n    {answer_b}\n    \n    Answer C:\n    {answer_c}\n    \n    Task:\n    1. Compare the three answers for accuracy, clarity, and completeness.\n    2. format and length of the answers are not important, focus on content quality.\n    3. Provide a short commentary.\n    4. Assign a score from 0 to 100 for Model A, Model B, and Model C.\n    5. Declare the overall winner.\n    \n    Output Format:\n    Commentary: <text>\n    Winner: <Model A or Model B or Model C>\n    Score A: <number>\n    Score B: <number>\n    Score C: <number>\n\"\"\")\n\n# Step 3: Combine everything into a full chain\n# The output of parallel_responses is a dict, which matches the input expected by judge_prompt\nfull_chain = parallel_responses | {\n    \"judgment\": judge_prompt | model_judge | StrOutputParser(),\n    \"answer_a\": lambda x: x[\"answer_a\"],\n    \"answer_b\": lambda x: x[\"answer_b\"],\n    \"answer_c\": lambda x: x[\"answer_c\"],\n    \"question\": lambda x: x[\"question\"]\n}\n```\n:::\n\n\n## 4. Execution\n\nFinally, we run the chain with a sample question.\n\n::: {#445b62f8 .cell execution_count=15}\n``` {.python .cell-code}\ninput_question = \"What is the AI advantage of using transformer architectures over traditional RNNs in natural language processing tasks?\"\n\nprint(f\"--- Running Workflow for Question: {input_question} ---\")\n\n# Execute the chain\nresult = full_chain.invoke({\"question\": input_question})\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n--- Running Workflow for Question: What is the AI advantage of using transformer architectures over traditional RNNs in natural language processing tasks? ---\n```\n:::\n:::\n\n\n## 5. Display Results\n\n::: {#57f885a3 .cell execution_count=16}\n``` {.python .cell-code}\nprint(\"\\n\" + \"=\"*40)\nprint(f\"QUESTION: {result['question']}\")\nprint(\"=\"*40)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n========================================\nQUESTION: {'question': 'What is the AI advantage of using transformer architectures over traditional RNNs in natural language processing tasks?'}\n========================================\n```\n:::\n:::\n\n\n### Model A\n\n::: {#e93e90bb .cell execution_count=17}\n``` {.python .cell-code}\nprint(f\"MODEL: {MODEL_A_NAME}\")\nprint(f\"\\n[Model A]:\\n{result.get('answer_a', 'No response')}\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMODEL: openai/gpt-oss-20b\n\n[Model A]:\n**The key AI advantage of transformers over traditional RNNs in NLP is that they fundamentally change how language information is represented, extracted, and learned, giving practitioners faster, more powerful, and more scalable models. Below is a concise map of those advantages:**\n\n| Aspect | RNN (e.g., LSTM/GRU) | Transformer | Why it matters for NLP |\n|--------|---------------------|------------|------------------------|\n| **Dependency capture** | Hidden state must carry all past context → *suffer from vanishing/exploding gradients* → long‑range dependencies are hard to learn. | **Self‑attention** lets every token directly attend to every other token, independent of distance. | Accurate capture of long‑range syntax & discourse (e.g., coreference, complex clauses). |\n| **Parallelism** | Sequential processing; layer‑by‑layer → inference and training can’t exploit GPU parallelism beyond a single time step. | Entire sequence processed in a single forward pass; all attention operations are **parallelizable** across tokens. | Greatly reduces training time on large corpora and enables larger batch sizes. |\n| **Gradient flow & stability** | Recurrent chain makes gradients decay/explode; training deeper RNNs requires tricks (tanh, peephole gates, etc.). | Transformers have **feed‑forward layers plus residual connections**; gradients flow more cleanly, allowing very deep stacks (30‑+ layers). | Allows building deeper, richer models that learn more nuanced linguistic patterns. |\n| **Scalability to large data** | Training a large RNN is time‑consuming; it’s hard to scale up model size without hitting memory/time limits. | Transformers scale almost linearly with data: *pre‑train* on billions of tokens and *fine‑tune* on task‑specific data. | Laid the foundation for “scaling laws” experiments and massively pretrained language models (GPT‑4, BERT, etc.). |\n| **Multi‑headed attention** | Single hidden state per time step → limited expressiveness of relations between tokens. | **Multiple attention heads** capture different relational patterns simultaneously (subject‑verb, adjective‑noun, disambiguation cues). | Improves performance on nuance‑rich tasks like sentiment, question answering, and disambiguation. |\n| **Position awareness** | Hidden state implicitly encodes order, but the representation is entangled with content. | **Positional encodings** (sinusoidal or learned) are added explicitly to token embeddings, letting attention focus on content while still respecting relative order. | Makes it trivial to adapt to different lengths and maintain positional invariance. |\n| **Modular versatility** | Single recurrent stack – harder to swap out or extend. | Transformer blocks are **easy to reuse**: encoder, decoder, cross‑attention, memory‑augmented variants, hierarchical transformers, etc. | Enables single architecture to handle translation, summarization, QA, and even multimodal tasks. |\n| **Inference efficiency** | Tensor‑ by‑tensor hidden state update → slower at inference, especially for long outputs. | For sequence generation you can reuse cached attention keys & values, and inference can be batched across many tokens. | Faster generation in real‑time applications. |\n\n### A concrete picture\n\n- **Long‑form documents**: An RNN would have to “compress” thousands of tokens into a single hidden vector, losing nuance. A transformer can pay attention to every part of the text irrespective of distance, so it can merge evidence from the opening, thesis, and conclusion all at once.\n- **Parallel training**: GPT‑3 was trained on 45 TB of text in ~few weeks on 256 GPUs because every layer of the transformer was parallelizable; the same RNN setup would take an order of magnitude longer.\n- **Fine‑tuning a huge pretrained base**: Because the transformer layers are identical and modular, you can fine‑tune just the final linear heads while re‑using the entire encoder‑decoder stack for many downstream tasks—a workflow that is almost impossible with bespoke RNNs.\n\n### Bottom line\n\nTransformers provide *global*, *parallel*, *stable*, and *scalable* modeling of language signals. These traits give them a decisive edge in modern NLP benchmarks and production systems, whereas classic RNNs remain useful only for limited‑length, low‑resource scenarios or when computational cost constraints preclude transformer training.\n```\n:::\n:::\n\n\n### Model B\n\n::: {#e5427ef0 .cell execution_count=18}\n``` {.python .cell-code}\nprint(f\"MODEL: {MODEL_B_NAME}\")\nprint(f\"\\n[Model B]:\\n{result.get('answer_b', 'No response')}\")\nprint(\"=\"*40)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMODEL: deepseek/deepseek-v3.2\n\n[Model B]:\nTransformer architectures provide several key AI advantages over traditional RNNs for natural language processing tasks:\n\n**Parallelization** – Unlike RNNs, which process sequences step-by-step, transformers use self-attention to analyze all words in a sequence simultaneously, enabling much faster training through parallel computation.\n\n**Long-range dependencies** – RNNs tend to struggle with long sequences due to vanishing/exploding gradient problems. Transformers maintain strong connections across any distance in a sequence via attention mechanisms.\n\n**Scalability** – Transformers scale more efficiently with larger datasets and model sizes, leading to breakthroughs in large language models (like GPT and BERT).\n\n**Attention mechanisms** – The self-attention mechanism allows the model to weigh the importance of different words in a sequence regardless of their positional distance, capturing richer contextual relationships.\n\n**No sequential dependency** – RNNs require previous hidden states to compute current ones, creating a computational bottleneck. Transformers remove this constraint entirely.\n\nThese advantages have made transformers the dominant architecture for modern NLP tasks, from machine translation and text generation to question answering and text classification.\n========================================\n```\n:::\n:::\n\n\n### Model C\n\n::: {#7a9108a8 .cell execution_count=19}\n``` {.python .cell-code}\nprint(f\"MODEL: {MODEL_C_NAME}\")\nprint(f\"\\n[Model C]:\\n{result.get('answer_c', 'No response')}\")\nprint(\"=\"*40)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMODEL: x-ai/grok-4.1-fast\n\n[Model C]:\n**Transformer architectures offer several key advantages over traditional RNNs (like LSTMs or GRUs) in NLP tasks:**\n\n### 1. **Parallelization and Training Efficiency**\n   - RNNs process sequences **sequentially** (one token at a time), limiting parallelism and making training slow on GPUs/TPUs, especially for long sequences.\n   - Transformers use **self-attention** to process the **entire sequence in parallel**, enabling massive speedups (e.g., BERT/GPT models train orders of magnitude faster than equivalent RNNs).\n\n### 2. **Better Handling of Long-Range Dependencies**\n   - RNNs struggle with **vanishing/exploding gradients**, making it hard to capture relationships between distant tokens (e.g., in long sentences or documents).\n   - Transformers' **attention mechanisms** directly compute dependencies between **all pairs of tokens** regardless of distance, excelling in tasks like machine translation, summarization, and question answering.\n\n### 3. **Scalability and Performance**\n   - Transformers scale effectively with **more layers, parameters, data, and compute** (e.g., leading to models like GPT-4 or T5 that dominate benchmarks).\n   - RNNs hit **diminishing returns** due to sequential bottlenecks and are outperformed on modern NLP leaderboards (e.g., GLUE, SuperGLUE).\n\n### 4. **Flexibility and Architectural Simplicity**\n   - No need for recurrence or hand-crafted gates; transformers are **purely attention-based**, easier to implement, modify (e.g., adding positional encodings), and extend to multimodal tasks.\n   - They avoid RNN-specific issues like **exposure bias** in autoregressive generation.\n\n**Empirical Evidence**: The 2017 Transformer paper (\"Attention is All You Need\") showed 2-3x faster training and 60 SOTA improvements on WMT translation. Today, nearly all top NLP models (BERT, RoBERTa, T5) are transformer-based, while RNNs are largely legacy.\n\nIn short, transformers revolutionized NLP by being **faster, more powerful, and scalable** for real-world sequence modeling.\n========================================\n```\n:::\n:::\n\n\n### AI Judge Verdict\n\n::: {#0b96f569 .cell execution_count=20}\n``` {.python .cell-code}\nprint(\"\\n>>> JUDGE'S VERDICT <<<\")\nprint(f\"MODEL_A: {MODEL_A_NAME}\")\nprint(f\"MODEL_B: {MODEL_B_NAME}\")\nprint(f\"MODEL_C: {MODEL_C_NAME}\")\nprint(f\"JUDGE:   {MODEL_JUDGE_NAME}\")\nprint(\"=====\")\nprint(result['judgment'])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n>>> JUDGE'S VERDICT <<<\nMODEL_A: openai/gpt-oss-20b\nMODEL_B: deepseek/deepseek-v3.2\nMODEL_C: x-ai/grok-4.1-fast\nJUDGE:   google/gemini-3-flash-preview\n=====\nCommentary: Model A is the most comprehensive and high-quality response. It uses a structured table to compare specific technical aspects (such as gradient flow, multi-headed attention, and position awareness) and provides concrete real-world examples to illustrate why these technical differences matter in practice. Model C is also excellent, offering clear headings and empirical evidence from the original \"Attention is All You Need\" paper. Model B is accurate and concise but lacks the depth and specific technical explanations (like residual connections or positional encodings) found in the other two.\n\nWinner: Model A\n\nScore A: 98\nScore B: 82\nScore C: 92\n```\n:::\n:::\n\n\n# n8n\n\n\n![](images/clipboard-1254380001.png)\n\n\n## json file\n\n::: {#92de97d2 .cell execution_count=21}\n``` {.python .cell-code}\n{\n  \"name\": \"AI Judge Workflow\",\n  \"nodes\": [\n    {\n      \"parameters\": {\n        \"assignments\": {\n          \"assignments\": [\n            {\n              \"id\": \"question-field\",\n              \"name\": \"question\",\n              \"value\": \"what is AI?\",\n              \"type\": \"string\"\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"id\": \"30f77dc6-8e05-4ddb-8902-1638b09abe7b\",\n      \"name\": \"Set Question1\",\n      \"type\": \"n8n-nodes-base.set\",\n      \"typeVersion\": 3.4,\n      \"position\": [\n        -688,\n        16\n      ]\n    },\n    {\n      \"parameters\": {\n        \"method\": \"POST\",\n        \"url\": \"https://openrouter.ai/api/v1/chat/completions\",\n        \"authentication\": \"genericCredentialType\",\n        \"genericAuthType\": \"httpBearerAuth\",\n        \"sendHeaders\": true,\n        \"headerParameters\": {\n          \"parameters\": [\n            {\n              \"name\": \"Content-Type\",\n              \"value\": \"application/json\"\n            }\n          ]\n        },\n        \"sendBody\": true,\n        \"specifyBody\": \"json\",\n        \"jsonBody\": \"={{ JSON.stringify({\\n  \\\"model\\\": \\\"openai/gpt-oss-120b:free\\\",\\n  \\\"messages\\\": [\\n    {\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"If you do not know the answer then reply I am not sure.\\\"},\\n    {\\\"role\\\": \\\"user\\\", \\\"content\\\": $json.question}\\n  ]\\n}) }}\",\n        \"options\": {}\n      },\n      \"id\": \"104becd7-9b3d-4297-b4ab-47b1df1abd71\",\n      \"name\": \"Model A\",\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"typeVersion\": 4.2,\n      \"position\": [\n        -480,\n        -160\n      ],\n      \"credentials\": {\n        \"httpHeaderAuth\": {\n          \"id\": \"Lf6835mjFnfNIqw6\",\n          \"name\": \"Header Auth account 2\"\n        },\n        \"httpBearerAuth\": {\n          \"id\": \"KFNalQ91U3mlnQks\",\n          \"name\": \"Bearer Auth account\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"method\": \"POST\",\n        \"url\": \"https://openrouter.ai/api/v1/chat/completions\",\n        \"authentication\": \"genericCredentialType\",\n        \"genericAuthType\": \"httpBearerAuth\",\n        \"sendHeaders\": true,\n        \"headerParameters\": {\n          \"parameters\": [\n            {\n              \"name\": \"Content-Type\",\n              \"value\": \"application/json\"\n            }\n          ]\n        },\n        \"sendBody\": true,\n        \"specifyBody\": \"json\",\n        \"jsonBody\": \"={{ JSON.stringify({\\n  \\\"model\\\": \\\"openai/gpt-oss-20b\\\",\\n  \\\"messages\\\": [\\n    {\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"If you do not know the answer then reply I am not sure.\\\"},\\n    {\\\"role\\\": \\\"user\\\", \\\"content\\\": $json.question}\\n  ]\\n}) }}\",\n        \"options\": {}\n      },\n      \"id\": \"2c4d3906-bcd3-4fc6-b983-4e9e494e476f\",\n      \"name\": \"Model B\",\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"typeVersion\": 4.2,\n      \"position\": [\n        -480,\n        16\n      ],\n      \"credentials\": {\n        \"httpHeaderAuth\": {\n          \"id\": \"Lf6835mjFnfNIqw6\",\n          \"name\": \"Header Auth account 2\"\n        },\n        \"httpBearerAuth\": {\n          \"id\": \"KFNalQ91U3mlnQks\",\n          \"name\": \"Bearer Auth account\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"method\": \"POST\",\n        \"url\": \"https://openrouter.ai/api/v1/chat/completions\",\n        \"authentication\": \"genericCredentialType\",\n        \"genericAuthType\": \"httpBearerAuth\",\n        \"sendHeaders\": true,\n        \"headerParameters\": {\n          \"parameters\": [\n            {\n              \"name\": \"Content-Type\",\n              \"value\": \"application/json\"\n            }\n          ]\n        },\n        \"sendBody\": true,\n        \"specifyBody\": \"json\",\n        \"jsonBody\": \"={{ JSON.stringify({\\n  \\\"model\\\": \\\"arcee-ai/trinity-large-preview:free\\\",\\n  \\\"messages\\\": [\\n    {\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"If you do not know the answer then reply I am not sure.\\\"},\\n    {\\\"role\\\": \\\"user\\\", \\\"content\\\": $json.question}\\n  ]\\n}) }}\",\n        \"options\": {}\n      },\n      \"id\": \"3f5e4a8b-c9d2-4ab7-a123-456789012345\",\n      \"name\": \"Model C\",\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"typeVersion\": 4.2,\n      \"position\": [\n        -480,\n        192\n      ],\n      \"credentials\": {\n        \"httpHeaderAuth\": {\n          \"id\": \"Lf6835mjFnfNIqw6\",\n          \"name\": \"Header Auth account 2\"\n        },\n        \"httpBearerAuth\": {\n          \"id\": \"KFNalQ91U3mlnQks\",\n          \"name\": \"Bearer Auth account\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"assignments\": {\n          \"assignments\": [\n            {\n              \"id\": \"map-a\",\n              \"name\": \"answer_a\",\n              \"value\": \"={{ $json.choices[0].message.content }}\",\n              \"type\": \"string\"\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"id\": \"bb423cd5-28ff-47e5-8925-492143d26228\",\n      \"name\": \"Set Answer A1\",\n      \"type\": \"n8n-nodes-base.set\",\n      \"typeVersion\": 3.4,\n      \"position\": [\n        -256,\n        -160\n      ]\n    },\n    {\n      \"parameters\": {\n        \"assignments\": {\n          \"assignments\": [\n            {\n              \"id\": \"map-b\",\n              \"name\": \"answer_b\",\n              \"value\": \"={{ $json.choices[0].message.content }}\",\n              \"type\": \"string\"\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"id\": \"591d8be5-132d-45a2-a69d-455467da46e5\",\n      \"name\": \"Set Answer B1\",\n      \"type\": \"n8n-nodes-base.set\",\n      \"typeVersion\": 3.4,\n      \"position\": [\n        -256,\n        16\n      ]\n    },\n    {\n      \"parameters\": {\n        \"assignments\": {\n          \"assignments\": [\n            {\n              \"id\": \"map-c\",\n              \"name\": \"answer_c\",\n              \"value\": \"={{ $json.choices[0].message.content }}\",\n              \"type\": \"string\"\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"id\": \"7a8b9c0d-1e2f-4a3b-8c4d-5e6f7a8b9c0d\",\n      \"name\": \"Set Answer C1\",\n      \"type\": \"n8n-nodes-base.set\",\n      \"typeVersion\": 3.4,\n      \"position\": [\n        -256,\n        192\n      ]\n    },\n    {\n      \"parameters\": {\n        \"mode\": \"combine\",\n        \"combineBy\": \"combineAll\",\n        \"options\": {}\n      },\n      \"id\": \"merge-ab-node-id\",\n      \"name\": \"Merge AB\",\n      \"type\": \"n8n-nodes-base.merge\",\n      \"typeVersion\": 3,\n      \"position\": [\n        -32,\n        -72\n      ]\n    },\n    {\n      \"parameters\": {\n        \"mode\": \"combine\",\n        \"combineBy\": \"combineAll\",\n        \"options\": {}\n      },\n      \"id\": \"2c89b328-cac3-4322-bcf6-b8ae5f9c0af2\",\n      \"name\": \"Merge Answers1\",\n      \"type\": \"n8n-nodes-base.merge\",\n      \"typeVersion\": 3,\n      \"position\": [\n        160,\n        16\n      ]\n    },\n    {\n      \"parameters\": {\n        \"method\": \"POST\",\n        \"url\": \"https://openrouter.ai/api/v1/chat/completions\",\n        \"authentication\": \"genericCredentialType\",\n        \"genericAuthType\": \"httpBearerAuth\",\n        \"sendHeaders\": true,\n        \"headerParameters\": {\n          \"parameters\": [\n            {\n              \"name\": \"Content-Type\",\n              \"value\": \"application/json\"\n            }\n          ]\n        },\n        \"sendBody\": true,\n        \"specifyBody\": \"json\",\n        \"jsonBody\": \"={{ JSON.stringify({\\n  \\\"model\\\": \\\"google/gemini-3-flash-preview\\\",\\n  \\\"messages\\\": [\\n    {\\n      \\\"role\\\": \\\"user\\\",\\n      \\\"content\\\": \\\"You are an AI Judge. You will be presented with a question and three candidate answers (Model A, Model B, and Model C).\\\\nYour task is to judge the quality of the answers.\\\\n\\\\nQuestion: \\\" + $(\\\"Set Question1\\\").first().json.question + \\\"\\\\n\\\\nAnswer A:\\\\n\\\" + ($json.answer_a || \\\"No answer provided\\\") + \\\"\\\\n\\\\nAnswer B:\\\\n\\\" + ($json.answer_b || \\\"No answer provided\\\") + \\\"\\\\n\\\\nAnswer C:\\\\n\\\" + ($json.answer_c || \\\"No answer provided\\\") + \\\"\\\\n\\\\nTask:\\\\n1. Compare the three answers for accuracy, clarity, and completeness.\\\\n2. format and length of the answers are not important, focus on content quality.\\\\n3. Provide a short commentary.\\\\n4. Assign a score from 0 to 100 for Model A, Model B, and Model C.\\\\n5. Declare the overall winner.\\\\n\\\\nOutput Format:\\\\nCommentary: <text>\\\\nWinner: <Model A or Model B or Model C>\\\\nScore A: <number>\\\\nScore B: <number>\\\\nScore C: <number>\\\"\\n    }\\n  ]\\n}) }}\",\n        \"options\": {}\n      },\n      \"id\": \"1d6171c9-4800-4f25-97a0-c029053e1ddc\",\n      \"name\": \"AI Judge1\",\n      \"type\": \"n8n-nodes-base.httpRequest\",\n      \"typeVersion\": 4.2,\n      \"position\": [\n        368,\n        16\n      ],\n      \"credentials\": {\n        \"httpHeaderAuth\": {\n          \"id\": \"Lf6835mjFnfNIqw6\",\n          \"name\": \"Header Auth account 2\"\n        },\n        \"httpBearerAuth\": {\n          \"id\": \"KFNalQ91U3mlnQks\",\n          \"name\": \"Bearer Auth account\"\n        }\n      }\n    },\n    {\n      \"parameters\": {\n        \"assignments\": {\n          \"assignments\": [\n            {\n              \"id\": \"final-result\",\n              \"name\": \"verdict\",\n              \"value\": \"={{ $json.choices[0].message.content }}\",\n              \"type\": \"string\"\n            },\n            {\n              \"id\": \"model-a-ans\",\n              \"name\": \"answer_a\",\n              \"value\": \"={{ $node[\\\"Merge Answers1\\\"].json.answer_a }}\",\n              \"type\": \"string\"\n            },\n            {\n              \"id\": \"model-b-ans\",\n              \"name\": \"answer_b\",\n              \"value\": \"={{ $node[\\\"Merge Answers1\\\"].json.answer_b }}\",\n              \"type\": \"string\"\n            },\n            {\n              \"id\": \"model-c-ans\",\n              \"name\": \"answer_c\",\n              \"value\": \"={{ $node[\\\"Merge Answers1\\\"].json.answer_c }}\",\n              \"type\": \"string\"\n            }\n          ]\n        },\n        \"options\": {}\n      },\n      \"id\": \"eae7f7bf-07fc-4250-b628-1a422cfbbe12\",\n      \"name\": \"Final Output1\",\n      \"type\": \"n8n-nodes-base.set\",\n      \"typeVersion\": 3.4,\n      \"position\": [\n        576,\n        16\n      ]\n    }\n  ],\n  \"pinData\": {},\n  \"connections\": {\n    \"Set Question1\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Model A\",\n            \"type\": \"main\",\n            \"index\": 0\n          },\n          {\n            \"node\": \"Model B\",\n            \"type\": \"main\",\n            \"index\": 0\n          },\n          {\n            \"node\": \"Model C\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Model A\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Set Answer A1\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Model B\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Set Answer B1\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Model C\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Set Answer C1\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Set Answer A1\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Merge AB\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Set Answer B1\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Merge AB\",\n            \"type\": \"main\",\n            \"index\": 1\n          }\n        ]\n      ]\n    },\n    \"Set Answer C1\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Merge Answers1\",\n            \"type\": \"main\",\n            \"index\": 1\n          }\n        ]\n      ]\n    },\n    \"Merge AB\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Merge Answers1\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"Merge Answers1\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"AI Judge1\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    },\n    \"AI Judge1\": {\n      \"main\": [\n        [\n          {\n            \"node\": \"Final Output1\",\n            \"type\": \"main\",\n            \"index\": 0\n          }\n        ]\n      ]\n    }\n  },\n  \"active\": false,\n  \"settings\": {\n    \"executionOrder\": \"v1\",\n    \"availableInMCP\": false\n  },\n  \"versionId\": \"ac74f20b-72a9-49a6-8200-fd08707946ec\",\n  \"meta\": {\n    \"instanceId\": \"2f42f1cdfcab2c6a7bbd5cec68912930ccc0107686e3a7211ddcc09504c524d9\"\n  },\n  \"id\": \"8a_wyeYhdVZSr42SS5Z1K\",\n  \"tags\": []\n}\n```\n:::\n\n\n:::\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}