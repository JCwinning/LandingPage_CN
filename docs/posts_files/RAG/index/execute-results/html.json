{
  "hash": "9d62db95967ec3fc4d9d36a43ecddde4",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"R 和 Python 中的检索增强生成 (RAG)\"\nauthor: \"Tony D\"\ndate: \"2025-11-02\"\ncategories: [AI, API, tutorial]\nimage: \"images.png\"\n\nformat:\n  html:\n    code-fold: true\n    code-tools: true\n    code-copy: true\n\nexecute:\n\n  warning: false\n---\n\n\n\n\n\n\n# 简介\n\n检索增强生成 (Retrieval-Augmented Generation, RAG) 是一种强大的技术，它将大型语言模型 (LLM) 的生成能力与信息检索的精准性相结合。通过将 LLM 的响应锚定在外部、可验证的数据中，RAG 减少了幻觉，并使模型能够回答关于特定、私有或最新信息的问题。\n\n在本教程中，我们将使用 R 和 Python 构建一个 RAG 系统。\n\n在 R 中，我们将利用 `ragnar` 包处理 RAG 工作流，并使用 `ellmer` 提供聊天界面。\n\n在 Python 中，我们将使用 `LangChain` 构建 RAG 流水线，使用 `ChromaDB` 作为向量数据库，并使用 `OpenAI` 进行模型交互。\n\n我们的目标是创建一个系统，通过爬取 OpenRouter API 的文档，来回答与其相关的问题。\n\n# 数据采集\n\n::: {.panel-tabset}\n\n\n## R\n\n首先，我们需要为知识库收集数据。我们将使用 `rvest` 包从 OpenRouter 文档中爬取 URL。这将为我们提供待接入的页面列表。\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ragnar)\nlibrary(ellmer)\nlibrary(dotenv)\nload_dot_env(file = \".env\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rvest)\n\n# 待爬取的 URL\nurl <- \"https://openrouter.ai/docs/quickstart\"\n\n# 读取页面的 HTML 内容\npage <- read_html(url)\n\n# 提取所有带有 href 的 <a> 标签\nlinks <- page %>%\n    html_nodes(\"a\") %>%\n    html_attr(\"href\")\n\n# 移除空值和重复项\nlinks <- unique(na.omit(links))\n\n# 可选：仅保留完整 URL\nlinks_full <- paste0(\"https://openrouter.ai\", links[grepl(\"^/docs/\", links)])\n\n# 打印所有链接\nprint(links_full)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"https://openrouter.ai/docs/api-reference/overview\"                               \n [2] \"https://openrouter.ai/docs/quickstart\"                                           \n [3] \"https://openrouter.ai/docs/api/reference/overview\"                               \n [4] \"https://openrouter.ai/docs/sdks/agentic-usage\"                                   \n [5] \"https://openrouter.ai/docs/guides/overview/principles\"                           \n [6] \"https://openrouter.ai/docs/guides/overview/models\"                               \n [7] \"https://openrouter.ai/docs/faq\"                                                  \n [8] \"https://openrouter.ai/docs/guides/overview/report-feedback\"                      \n [9] \"https://openrouter.ai/docs/guides/routing/model-fallbacks\"                       \n[10] \"https://openrouter.ai/docs/guides/routing/provider-selection\"                    \n[11] \"https://openrouter.ai/docs/guides/features/presets\"                              \n[12] \"https://openrouter.ai/docs/guides/features/tool-calling\"                         \n[13] \"https://openrouter.ai/docs/guides/features/structured-outputs\"                   \n[14] \"https://openrouter.ai/docs/guides/features/message-transforms\"                   \n[15] \"https://openrouter.ai/docs/guides/features/zero-completion-insurance\"            \n[16] \"https://openrouter.ai/docs/guides/features/zdr\"                                  \n[17] \"https://openrouter.ai/docs/app-attribution\"                                      \n[18] \"https://openrouter.ai/docs/guides/features/guardrails\"                           \n[19] \"https://openrouter.ai/docs/faq#how-are-rate-limits-calculated\"                   \n[20] \"https://openrouter.ai/docs/api/reference/streaming\"                              \n[21] \"https://openrouter.ai/docs/guides/community/frameworks-and-integrations-overview\"\n```\n\n\n:::\n:::\n\n\n\n## Python \n\n首先，我们需要为知识库收集数据。我们将使用 `requests` 和 `BeautifulSoup` 从 OpenRouter 文档中爬取 URL。这将为我们提供待接入的页面列表。\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport sys\nprint(sys.executable)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n/Library/Frameworks/Python.framework/Versions/3.13/bin/python3\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport requests\nfrom bs4 import BeautifulSoup\nfrom dotenv import load_dotenv\nimport os\nfrom markitdown import MarkItDown\nfrom io import BytesIO\nimport re\n\n# 加载环境变量\nload_dotenv()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue\n```\n\n\n:::\n\n```{.python .cell-code}\n# 辅助函数\ndef fetch_html(url: str) -> bytes:\n    \"\"\"从 URL 获取 HTML 内容并以字节形式返回。\"\"\"\n    resp = requests.get(url)\n    resp.raise_for_status()\n    return resp.content\n\ndef html_to_markdown(html_bytes: bytes) -> str:\n    \"\"\"使用 MarkItDown 将 HTML 字节转换为 Markdown。\"\"\"\n    md = MarkItDown()\n    stream = BytesIO(html_bytes)\n    result = md.convert_stream(stream, mime_type=\"text/html\")\n    return result.markdown\n\ndef save_markdown(md_content: str, output_path: str):\n    \"\"\"将 Markdown 内容保存到文件。\"\"\"\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(md_content)\n\ndef sanitize_filename(filename: str) -> str:\n    \"\"\"清理 URL 以创建合法的文件名。\"\"\"\n    filename = re.sub(r'^https?://[^/]+', '', filename)\n    filename = re.sub(r'[^\\w\\-_.]', '_', filename)\n    filename = filename.strip('_')\n    if not filename.endswith('.md'):\n        filename += '.md'\n    return filename\n\n# 待爬取的 URL\nurl = \"https://openrouter.ai/docs/quickstart\"\n\n# 读取页面的 HTML 内容\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.content, 'html.parser')\n\n# 提取所有带有 href 的 <a> 标签\nlinks = [a['href'] for a in soup.find_all('a', href=True)]\n\n# 移除重复项\nlinks = list(set(links))\n\n# 仅保留文档的完整 URL\nlinks_full = [f\"https://openrouter.ai{link}\" for link in links if link.startswith(\"/docs/\")]\n\n# 显式添加 FAQ\nlinks_full.append(\"https://openrouter.ai/docs/faq\")\nlinks_full = list(set(links_full))\n\n# 打印所有链接\nprint(f\"找到 {len(links_full)} 个文档 URL\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n找到 21 个文档 URL\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(links_full)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n['https://openrouter.ai/docs/faq', 'https://openrouter.ai/docs/sdks/agentic-usage', 'https://openrouter.ai/docs/guides/features/message-transforms', 'https://openrouter.ai/docs/api/reference/overview', 'https://openrouter.ai/docs/guides/overview/principles', 'https://openrouter.ai/docs/guides/overview/report-feedback', 'https://openrouter.ai/docs/api-reference/overview', 'https://openrouter.ai/docs/quickstart', 'https://openrouter.ai/docs/guides/community/frameworks-and-integrations-overview', 'https://openrouter.ai/docs/guides/routing/provider-selection', 'https://openrouter.ai/docs/guides/features/structured-outputs', 'https://openrouter.ai/docs/faq#how-are-rate-limits-calculated', 'https://openrouter.ai/docs/guides/features/presets', 'https://openrouter.ai/docs/guides/routing/model-fallbacks', 'https://openrouter.ai/docs/guides/features/guardrails', 'https://openrouter.ai/docs/api/reference/streaming', 'https://openrouter.ai/docs/guides/features/zdr', 'https://openrouter.ai/docs/app-attribution', 'https://openrouter.ai/docs/guides/features/tool-calling', 'https://openrouter.ai/docs/guides/overview/models', 'https://openrouter.ai/docs/guides/features/zero-completion-insurance']\n```\n\n\n:::\n:::\n\n\n\n:::\n\n# 将网页内容保存到本地\n\n::: {.panel-tabset}\n\n## R\n\n为了进行语义搜索，我们需要将文本数据存储为向量（嵌入）。我们将使用 `DuckDB` 作为本地向量数据库。我们还需要一个嵌入模型将文本转换为向量。在这里，我们配置 `ragnar` 通过 OpenAI 兼容的 API (SiliconFlow) 使用特定的嵌入模型。\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# pages <- ragnar_find_links(base_url)\npages <- links_full\nstore_location <- \"openrouter.duckdb\"\n\nstore <- ragnar_store_create(\n    store_location,\n    overwrite = TRUE,\n    embed = \\(x) ragnar::embed_openai(x,\n        model = \"BAAI/bge-m3\",\n        base_url = \"https://api.siliconflow.cn/v1\",\n        api_key = Sys.getenv(\"siliconflow\")\n    )\n)\n```\n:::\n\n\n\n在存储初始化后，我们现在可以接入数据。我们遍历之前爬取的页面列表。对于每个页面，我们：\n1. 以 Markdown 格式读取内容。\n2. 将内容拆分为较小的块（约 600 字符）。\n3. 将这些块插入到我们的向量数据库中。\n\n此过程构建了我们将要搜索的索引。\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# page=\"https://openrouter.ai/docs/faq\"\n# chunks <- page |>read_as_markdown() |>markdown_chunk(target_size = 2000)\n# ragnar_chunks_view(chunks)\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (page in pages) {\n    message(\"正在接入: \", page)\n    print(page)\n    chunks <- page |>\n        read_as_markdown() |>\n        markdown_chunk(target_size = 2000)\n    # print(chunks)\n    # print('chunks done')\n    ragnar_store_insert(store, chunks)\n    print(\"插入完成\")\n}\n```\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nragnar_store_build_index(store)\n\n# 释放连接以供后续 Python 代码使用\nrm(store)\ngc()\n```\n:::\n\n\n\n\n## Python DuckDB\n\n::: {.cell}\n\n```{.python .cell-code}\nimport os\nfrom llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, Settings\nfrom llama_index.vector_stores.duckdb import DuckDBVectorStore\nfrom llama_index.embeddings.openai import OpenAIEmbedding\n\n# --- 1. 配置 ---\n\n# 确保 API 密钥可用\nopenrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\") # 或直接粘贴字符串\n\n# 初始化指向 OpenRouter 的嵌入模型\n# 我们使用 OpenAI 类，因为 OpenRouter 使用了 OpenAI 兼容的 API 结构\nembed_model = OpenAIEmbedding(\n    api_key=openrouter_api_key,\n    base_url=\"https://openrouter.ai/api/v1\",\n    model=\"qwen/qwen3-embedding-8b\"  \n)\n\n# 更新全局设置，以便 LlamaIndex 知道使用此模型\nSettings.embed_model = embed_model\nSettings.chunk_size = 2000\nSettings.chunk_overlap = 200\n# --- 2. 接入与索引 ---\n\n# 加载数据\ndocuments = SimpleDirectoryReader(\"markdown_docs\").load_data()\n\n# 初始化 DuckDB 向量数据库\nvector_store = DuckDBVectorStore(\"openrouter.duckdb\", persist_dir=\"./persist/\")\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\n\n# 创建索引\n# 这将自动使用 Settings 中定义的 Qwen 嵌入\nindex = VectorStoreIndex.from_documents(\n    documents, \n    storage_context=storage_context\n)\n```\n:::\n\n\n\n\n\n\n## Python Chroma\n\n为了进行语义搜索，我们需要将文本数据转换为向量（嵌入）进行存储。我们将使用 `ChromaDB` 作为本地向量数据库。我们还需要一个嵌入模型把文本转为向量。在这里，我们配置了一个自定义的 `OpenRouterEmbeddings` 类，通过 OpenRouter API 使用 `qwen/qwen3-embedding-8b` 模型。\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom openai import OpenAI\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_chroma import Chroma\nfrom typing import List\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# 针对 OpenRouter API 的自定义嵌入类\nclass OpenRouterEmbeddings(Embeddings):\n    \"\"\"针对 OpenRouter API 的自定义嵌入类。\"\"\"\n    \n    def __init__(self, api_key: str, model: str = \"text-embedding-3-small\"):\n        self.client = OpenAI(\n            base_url=\"https://openrouter.ai/api/v1\",\n            api_key=api_key,\n        )\n        self.model = model\n    \n    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"对文档列表进行嵌入。\"\"\"\n        response = self.client.embeddings.create(\n            extra_headers={\n                \"HTTP-Referer\": \"https://ai-blog.com\",\n                \"X-Title\": \"AI Blog RAG\",\n            },\n            model=self.model,\n            input=texts,\n            encoding_format=\"float\"\n        )\n        return [item.embedding for item in response.data]\n    \n    def embed_query(self, text: str) -> List[float]:\n        \"\"\"对单个查询进行嵌入。\"\"\"\n        response = self.client.embeddings.create(\n            extra_headers={\n                \"HTTP-Referer\": \"https://ai-blog.com\",\n                \"X-Title\": \"AI Blog RAG\",\n            },\n            model=self.model,\n            input=text,\n            encoding_format=\"float\"\n        )\n        return response.data[0].embedding\n\n# 获取 OpenRouter API 密钥\nopenrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\nif not openrouter_api_key:\n    raise ValueError(\"未在环境变量中找到 OPENROUTER_API_KEY\")\n\n# 使用 OpenRouter 创建嵌入实例\nembeddings = OpenRouterEmbeddings(\n    api_key=openrouter_api_key,\n    model=\"qwen/qwen3-embedding-8b\"\n)\n\n# 定义向量数据库位置\npersist_directory = \"chroma_db_data\"\n```\n:::\n\n\n\n\n\n在存储初始化后，我们现在可以接入数据。我们遍历之前保存的 Markdown 文件。对于每个文件，我们：\n1. 加载内容。\n2. 使用 `RecursiveCharacterTextSplitter` 将内容拆分为较小的块（约 2000 字符）。\n3. 从这些块中创建一个新的 `Chroma` 向量数据库。\n\n此过程构建了我们将要搜索的索引。\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom langchain_core.documents import Document\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nimport shutil\n\n# 加载 Markdown 文件的辅助函数\ndef load_markdown_files(directory: str) -> list[Document]:\n    \"\"\"从目录加载所有 Markdown 文件并创建 Document 对象。\"\"\"\n    documents = []\n    if not os.path.exists(directory):\n        return documents\n        \n    for filename in os.listdir(directory):\n        if filename.endswith('.md'):\n            filepath = os.path.join(directory, filename)\n            with open(filepath, 'r', encoding='utf-8') as f:\n                content = f.read()\n                doc = Document(\n                    page_content=content,\n                    metadata={\n                        \"source\": filename,\n                        \"filepath\": filepath\n                    }\n                )\n                documents.append(doc)\n    return documents\n\n# 创建 Markdown 文件的输出目录\noutput_dir = \"markdown_docs\"\nos.makedirs(output_dir, exist_ok=True)\n\n# 将每个 URL 转换为 Markdown 并保存\nfor i, link_url in enumerate(links_full, 1):\n    try:\n        print(f\"正在处理 {i}/{len(links_full)}: {link_url}\")\n        html_content = fetch_html(link_url)\n        markdown_content = html_to_markdown(html_content)\n        filename = sanitize_filename(link_url)\n        output_path = os.path.join(output_dir, filename)\n        save_markdown(markdown_content, output_path)\n        print(f\"  ✓ 已保存至 {output_path}\")\n    except Exception as e:\n        print(f\"  ✗ 处理 {link_url} 时出错: {str(e)}\")\n\n# 加载 Markdown 文档\ndocuments = load_markdown_files(output_dir)\nprint(f\"\\n加载了 {len(documents)} 个 Markdown 文档\")\n\n# 将文档拆分为块\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=2000,\n    chunk_overlap=200,\n    length_function=len,\n    is_separator_regex=False,\n)\n\nsplits = text_splitter.split_documents(documents)\nprint(f\"拆分为 {len(splits)} 个块\")\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# 如果数据库已存在，则将其移除\nif os.path.exists(persist_directory):\n    print(f\"正在移除位于 {persist_directory} 的现有数据库...\")\n    shutil.rmtree(persist_directory)\n\n# 创建新的向量数据库\nvectorstore = Chroma.from_documents(\n    documents=splits,\n    embedding=embeddings,\n    persist_directory=persist_directory\n)\n\nprint(f\"\\n✓ 成功创建了包含 {len(splits)} 个块的 ChromaDB！\")\nprint(f\"✓ 数据库已保存至: {persist_directory}\")\n```\n:::\n\n\n\n\n:::\n\n\n\n\n\n\n\n\n\n\n# 检索\n\n::: {.panel-tabset}\n\n## R\n\n现在我们的知识库已经填充完毕，我们可以测试检索系统。我们可以提出一个特定的问题，例如“什么是模型变体？(What are model variants?)”，并查询存储库以查看哪些文本块最相关。这确认了我们的嵌入和搜索是否正常工作。\n\n### 问题：什么是模型变体？(What are model variants?)\n\nRAG 结果：\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstore_location <- \"openrouter.duckdb\"\ntext <- \"What are model variants?\"\n\nrelevant_chunks <- tryCatch({\n    store <- ragnar_store_connect(store_location)\n    ragnar_retrieve(store, text, top_k = 3)\n}, error = function(e) {\n    message(\"⚠️ 无法连接到 DuckDB (可能被锁定): \", e$message)\n    return(NULL)\n})\n\nif (!is.null(relevant_chunks)) {\n    cat(\"检索到\", nrow(relevant_chunks), \"个文本块：\\n\\n\")\n    for (i in seq_len(nrow(relevant_chunks))) {\n        cat(sprintf(\"--- 块 %d ---\\n%s\\n\\n\", i, relevant_chunks$text[i]))\n    }\n} else {\n    cat(\"知识库当前不可用（由于数据库锁定）。\")\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n````\n检索到 6 个文本块：\n\n--- 块 1 ---\n[Zero Completion Insurance](/docs/features/zero-completion-insurance)\n  + [Provisioning API Keys](/docs/features/provisioning-api-keys)\n  + [App Attribution](/docs/app-attribution)\n* API Reference\n\n  + [Overview](/docs/api-reference/overview)\n  + [Streaming](/docs/api-reference/streaming)\n  + [Embeddings](/docs/api-reference/embeddings)\n  + [Limits](/docs/api-reference/limits)\n  + [Authentication](/docs/api-reference/authentication)\n  + [Parameters](/docs/api-reference/parameters)\n  + [Errors](/docs/api-reference/errors)\n  + Responses API\n  + beta.responses\n  + Analytics\n  + Credits\n  + Embeddings\n  + Generations\n  + Models\n  + Endpoints\n  + Parameters\n  + Providers\n  + API Keys\n  + O Auth\n  + Chat\n  + Completions\n* SDK Reference (BETA)\n\n  + [Python SDK](/docs/sdks/python)\n  + [TypeScript SDK](/docs/sdks/typescript)\n* Use Cases\n\n  + [BYOK](/docs/use-cases/byok)\n  + [Crypto API](/docs/use-cases/crypto-api)\n  + [OAuth PKCE](/docs/use-cases/oauth-pkce)\n  + [MCP Servers](/docs/use-cases/mcp-servers)\n  + [Organization Management](/docs/use-cases/organization-management)\n  + [For Providers](/docs/use-cases/for-providers)\n  + [Reasoning Tokens](/docs/use-cases/reasoning-tokens)\n  + [Usage Accounting](/docs/use-cases/usage-accounting)\n  + [User Tracking](/docs/use-cases/user-tracking)\n* Community\n\n  + [Frameworks and Integrations Overview](/docs/community/frameworks-and-integrations-overview)\n  + [Effect AI SDK](/docs/community/effect-ai-sdk)\n  + [Arize](/docs/community/arize)\n  + [LangChain](/docs/community/lang-chain)\n  + [LiveKit](/docs/community/live-kit)\n  + [Langfuse](/docs/community/langfuse)\n  + [Mastra](/docs/community/mastra)\n  + [OpenAI SDK](/docs/community/open-ai-sdk)\n  + [PydanticAI](/docs/community/pydantic-ai)\n  + [Vercel AI SDK](/docs/community/vercel-ai-sdk)\n  + [Xcode](/docs/community/xcode)\n  + [Zapier](/docs/community/zapier)\n  + [Discord](https://discord.gg/openrouter)\n\nLight\n\nOn this page\n\n* [Requests](#requests)\n* [Completions Request Format](#completions-request-format)\n* [Headers](#headers)\n* [Assistant Prefill](#assistant-prefill)\n* [Responses](#responses)\n* [CompletionsResponse Format](#completionsresponse-format)\n* [Finish Reason](#finish-reason)\n* [Querying Cost and Stats](#querying-cost-and-stats)\n\n[API Reference](/docs/api-reference/overview)\n\n\n\n--- 块 2 ---\n###### How frequently are new models added?\n\nWe work on adding models as quickly as we can. We often have partnerships with\nthe labs releasing models and can release models as soon as they are\navailable. If there is a model missing that you’d like OpenRouter to support, feel free to message us on\n[Discord](https://discord.gg/openrouter).\n\n###### What are model variants?\n\nVariants are suffixes that can be added to the model slug to change its behavior.\n\nStatic variants can only be used with specific models and these are listed in our [models api](https://openrouter.ai/api/v1/models).\n\n1. `:free` - The model is always provided for free and has low rate limits.\n2. `:beta` - The model is not moderated by OpenRouter.\n3. `:extended` - The model has longer than usual context length.\n4. `:exacto` - The model only uses OpenRouter-curated high-quality endpoints.\n5. `:thinking` - The model supports reasoning by default.\n\nDynamic variants can be used on all models and they change the behavior of how the request is routed or used.\n\n1. `:online` - All requests will run a query to extract web results that are attached to the prompt.\n2. `:nitro` - Providers will be sorted by throughput rather than the default sort, optimizing for faster response times.\n3. `:floor` - Providers will be sorted by price rather than the default sort, prioritizing the most cost-effective options.\n\n###### I am an inference provider, how can I get listed on OpenRouter?\n\nYou can read our requirements at the [Providers\npage](/docs/use-cases/for-providers). If you would like to contact us, the best\nplace to reach us is over email.\n\n###### What is the expected latency/response time for different models?\n\nFor each model on OpenRouter we show the latency (time to first token) and the token\nthroughput for all providers. You can use this to estimate how long requests\nwill take. If you would like to optimize for throughput you can use the\n`:nitro` variant to route to the fastest provider.\n\n\n\n--- 块 3 ---\n###### How frequently are new models added?\n\nWe work on adding models as quickly as we can. We often have partnerships with\nthe labs releasing models and can release models as soon as they are\navailable. If there is a model missing that you’d like OpenRouter to support, feel free to message us on\n[Discord](https://discord.gg/openrouter).\n\n###### What are model variants?\n\nVariants are suffixes that can be added to the model slug to change its behavior.\n\nStatic variants can only be used with specific models and these are listed in our [models api](https://openrouter.ai/api/v1/models).\n\n1. `:free` - The model is always provided for free and has low rate limits.\n2. `:beta` - The model is not moderated by OpenRouter.\n3. `:extended` - The model has longer than usual context length.\n4. `:exacto` - The model only uses OpenRouter-curated high-quality endpoints.\n5. `:thinking` - The model supports reasoning by default.\n\nDynamic variants can be used on all models and they change the behavior of how the request is routed or used.\n\n1. `:online` - All requests will run a query to extract web results that are attached to the prompt.\n2. `:nitro` - Providers will be sorted by throughput rather than the default sort, optimizing for faster response times.\n3. `:floor` - Providers will be sorted by price rather than the default sort, prioritizing the most cost-effective options.\n\n###### I am an inference provider, how can I get listed on OpenRouter?\n\nYou can read our requirements at the [Providers\npage](/docs/use-cases/for-providers). If you would like to contact us, the best\nplace to reach us is over email.\n\n###### What is the expected latency/response time for different models?\n\nFor each model on OpenRouter we show the latency (time to first token) and the token\nthroughput for all providers. You can use this to estimate how long requests\nwill take. If you would like to optimize for throughput you can use the\n`:nitro` variant to route to the fastest provider.\n\n\n\n--- 块 4 ---\n## The `models` parameter\n\nThe `models` parameter lets you automatically try other models if the primary model’s providers are down, rate-limited, or refuse to reply due to content moderation.\n\nTypeScript SDKTypeScript (fetch)Python\n\n```code-block-root not-prose rounded-b-[inherit] rounded-t-none\n|  |  |\n| --- | --- |\n| 1 | import { OpenRouter } from '@openrouter/sdk'; |\n| 2 |  |\n| 3 | const openRouter = new OpenRouter({ |\n| 4 | apiKey: '<OPENROUTER_API_KEY>', |\n| 5 | }); |\n| 6 |  |\n| 7 | const completion = await openRouter.chat.send({ |\n| 8 | models: ['anthropic/claude-3.5-sonnet', 'gryphe/mythomax-l2-13b'], |\n| 9 | messages: [ |\n| 10 | { |\n| 11 | role: 'user', |\n| 12 | content: 'What is the meaning of life?', |\n| 13 | }, |\n| 14 | ], |\n| 15 | }); |\n| 16 |  |\n| 17 | console.log(completion.choices[0].message.content); |\n```\n\nIf the model you selected returns an error, OpenRouter will try to use the fallback model instead. If the fallback model is down or returns an error, OpenRouter will return that error.\n\nBy default, any error can trigger the use of a fallback model, including context length validation errors, moderation flags for filtered models, rate-limiting, and downtime.\n\nRequests are priced using the model that was ultimately used, which will be returned in the `model` attribute of the response body.\n\n## Using with OpenAI SDK\n\nTo use the `models` array with the OpenAI SDK, include it in the `extra_body` parameter. In the example below, gpt-4o will be tried first, and the `models` array will be tried in order as fallbacks.\n\nPythonTypeScript\n\n\n\n--- 块 5 ---\n[Web Search](/docs/features/web-search)\n  + [Zero Completion Insurance](/docs/features/zero-completion-insurance)\n  + [Provisioning API Keys](/docs/features/provisioning-api-keys)\n  + [App Attribution](/docs/app-attribution)\n* API Reference\n\n  + [Overview](/docs/api-reference/overview)\n  + [Streaming](/docs/api-reference/streaming)\n  + [Embeddings](/docs/api-reference/embeddings)\n  + [Limits](/docs/api-reference/limits)\n  + [Authentication](/docs/api-reference/authentication)\n  + [Parameters](/docs/api-reference/parameters)\n  + [Errors](/docs/api-reference/errors)\n  + Responses API\n  + beta.responses\n  + Analytics\n  + Credits\n  + Embeddings\n  + Generations\n  + Models\n  + Endpoints\n  + Parameters\n  + Providers\n  + API Keys\n  + O Auth\n  + Chat\n  + Completions\n* SDK Reference (BETA)\n\n  + [Python SDK](/docs/sdks/python)\n  + [TypeScript SDK](/docs/sdks/typescript)\n* Use Cases\n\n  + [BYOK](/docs/use-cases/byok)\n  + [Crypto API](/docs/use-cases/crypto-api)\n  + [OAuth PKCE](/docs/use-cases/oauth-pkce)\n  + [MCP Servers](/docs/use-cases/mcp-servers)\n  + [Organization Management](/docs/use-cases/organization-management)\n  + [For Providers](/docs/use-cases/for-providers)\n  + [Reasoning Tokens](/docs/use-cases/reasoning-tokens)\n  + [Usage Accounting](/docs/use-cases/usage-accounting)\n  + [User Tracking](/docs/use-cases/user-tracking)\n* Community\n\n  + [Frameworks and Integrations Overview](/docs/community/frameworks-and-integrations-overview)\n  + [Effect AI SDK](/docs/community/effect-ai-sdk)\n  + [Arize](/docs/community/arize)\n  + [LangChain](/docs/community/lang-chain)\n  + [LiveKit](/docs/community/live-kit)\n  + [Langfuse](/docs/community/langfuse)\n  + [Mastra](/docs/community/mastra)\n  + [OpenAI SDK](/docs/community/open-ai-sdk)\n  + [PydanticAI](/docs/community/pydantic-ai)\n  + [Vercel AI SDK](/docs/community/vercel-ai-sdk)\n  + [Xcode](/docs/community/xcode)\n  + [Zapier](/docs/community/zapier)\n  + [Discord](https://discord.gg/openrouter)\n\nLight\n\nOn this page\n\n* [Within OpenRouter](#within-openrouter)\n* [Provider Policies](#provider-policies)\n* [Training on Prompts](#training-on-prompts)\n* [Data Retention & Logging](#data-retention--logging)\n* [Enterprise EU in-region routing](#enterprise-eu-in-region-routing)\n\n[Features](/docs/features/privacy-and-logging)\n\n\n\n--- 块 6 ---\n[Web Search](/docs/features/web-search)\n  + [Zero Completion Insurance](/docs/features/zero-completion-insurance)\n  + [Provisioning API Keys](/docs/features/provisioning-api-keys)\n  + [App Attribution](/docs/app-attribution)\n* API Reference\n\n  + [Overview](/docs/api-reference/overview)\n  + [Streaming](/docs/api-reference/streaming)\n  + [Embeddings](/docs/api-reference/embeddings)\n  + [Limits](/docs/api-reference/limits)\n  + [Authentication](/docs/api-reference/authentication)\n  + [Parameters](/docs/api-reference/parameters)\n  + [Errors](/docs/api-reference/errors)\n  + Responses API\n  + beta.responses\n  + Analytics\n  + Credits\n  + Embeddings\n  + Generations\n  + Models\n  + Endpoints\n  + Parameters\n  + Providers\n  + API Keys\n  + O Auth\n  + Chat\n  + Completions\n* SDK Reference (BETA)\n\n  + [Python SDK](/docs/sdks/python)\n  + [TypeScript SDK](/docs/sdks/typescript)\n* Use Cases\n\n  + [BYOK](/docs/use-cases/byok)\n  + [Crypto API](/docs/use-cases/crypto-api)\n  + [OAuth PKCE](/docs/use-cases/oauth-pkce)\n  + [MCP Servers](/docs/use-cases/mcp-servers)\n  + [Organization Management](/docs/use-cases/organization-management)\n  + [For Providers](/docs/use-cases/for-providers)\n  + [Reasoning Tokens](/docs/use-cases/reasoning-tokens)\n  + [Usage Accounting](/docs/use-cases/usage-accounting)\n  + [User Tracking](/docs/use-cases/user-tracking)\n* Community\n\n  + [Frameworks and Integrations Overview](/docs/community/frameworks-and-integrations-overview)\n  + [Effect AI SDK](/docs/community/effect-ai-sdk)\n  + [Arize](/docs/community/arize)\n  + [LangChain](/docs/community/lang-chain)\n  + [LiveKit](/docs/community/live-kit)\n  + [Langfuse](/docs/community/langfuse)\n  + [Mastra](/docs/community/mastra)\n  + [OpenAI SDK](/docs/community/open-ai-sdk)\n  + [PydanticAI](/docs/community/pydantic-ai)\n  + [Vercel AI SDK](/docs/community/vercel-ai-sdk)\n  + [Xcode](/docs/community/xcode)\n  + [Zapier](/docs/community/zapier)\n  + [Discord](https://discord.gg/openrouter)\n\nLight\n\nOn this page\n\n* [How OpenRouter Manages Data Policies](#how-openrouter-manages-data-policies)\n* [Per-Request ZDR Enforcement](#per-request-zdr-enforcement)\n* [Usage](#usage)\n* [Caching](#caching)\n* [OpenRouter’s Retention Policy](#openrouters-retention-policy)\n* [Zero Retention Endpoints](#zero-retention-endpoints)\n\n[Features](/docs/features/privacy-and-logging)\n````\n\n\n:::\n\n```{.r .cell-code}\n# 释放连接以避免文件锁定\nif (exists(\"store\")) {\n    rm(store)\n    gc()\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n          used  (Mb) gc trigger  (Mb) limit (Mb) max used  (Mb)\nNcells 2385439 127.4    4675528 249.8         NA  3250438 173.6\nVcells 4239754  32.4   10146329  77.5      16384  5558189  42.5\n```\n\n\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ragnar_store_inspect(store)\n#ragnar_chunks_view(chunks)\n```\n:::\n\n\n## Python DuckDB\n\n在 Python 中，我们可以使用 `LlamaIndex` 与我们的 DuckDB 向量数据库进行交互。在此步骤中，我们将配置嵌入模型并为查询检索前几个相关块，并将它们保存到文件中以供检查。我们暂不使用 LLM 进行生成，仅专注于验证检索质量。\n\n### 问题：什么是模型变体？(What are model variants?)\n\nRAG 结果：\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport os\nimport sys\nprint(f\"Python 可执行文件路径: {sys.executable}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPython 可执行文件路径: /Library/Frameworks/Python.framework/Versions/3.13/bin/python3\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(f\"Python 路径 (sys.path): {sys.path}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPython 路径 (sys.path): ['', '/Library/Frameworks/Python.framework/Versions/3.13/bin', '/Library/Frameworks/Python.framework/Versions/3.13/lib/python313.zip', '/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13', '/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/lib-dynload', '/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages', '/Library/Frameworks/R.framework/Versions/4.5-arm64/Resources/library/reticulate/python']\n```\n\n\n:::\n\n```{.python .cell-code}\nfrom typing import Any, List\nfrom openai import OpenAI\nfrom llama_index.core import VectorStoreIndex, Settings\nfrom llama_index.core.embeddings import BaseEmbedding\nfrom llama_index.vector_stores.duckdb import DuckDBVectorStore\nfrom dotenv import load_dotenv\n\n# 加载环境变量\nload_dotenv()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue\n```\n\n\n:::\n\n```{.python .cell-code}\n# 确保 API 密钥可用\nopenrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n\n# 针对 LlamaIndex 的自定义 OpenRouter 嵌入类\nclass OpenRouterEmbedding(BaseEmbedding):\n    \"\"\"与 LlamaIndex 兼容的 OpenRouter API 自定义嵌入类。\"\"\"\n    \n    def __init__(\n        self,\n        api_key: str,\n        model: str = \"qwen/qwen3-embedding-8b\",\n        **kwargs: Any\n    ):\n        super().__init__(**kwargs)\n        self._client = OpenAI(\n            base_url=\"https://openrouter.ai/api/v1\",\n            api_key=api_key,\n        )\n        self._model = model\n    \n    def _get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"获取查询字符串的嵌入向量。\"\"\"\n        response = self._client.embeddings.create(\n            extra_headers={\n                \"HTTP-Referer\": \"https://ai-blog.com\",\n                \"X-Title\": \"AI Blog RAG\",\n            },\n            model=self._model,\n            input=query,\n            encoding_format=\"float\"\n        )\n        return response.data[0].embedding\n    \n    def _get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"获取文本字符串的嵌入向量。\"\"\"\n        return self._get_query_embedding(text)\n    \n    async def _aget_query_embedding(self, query: str) -> List[float]:\n        \"\"\"异步版本的 get_query_embedding。\"\"\"\n        return self._get_query_embedding(query)\n    \n    async def _aget_text_embedding(self, text: str) -> List[float]:\n        \"\"\"异步版本的 get_text_embedding。\"\"\"\n        return self._get_text_embedding(text)\n\n# 1. 使用自定义 OpenRouter 类配置嵌入模型\nembed_model = OpenRouterEmbedding(\n    api_key=openrouter_api_key,\n    model=\"qwen/qwen3-embedding-8b\"\n)\n\n# 2. 应用设置\nSettings.embed_model = embed_model\n\n# 加载与检索\n# 加载现有的 DuckDB 向量数据库\nprint(\"正在从 openrouter.duckdb 加载向量数据库...\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n正在从 openrouter.duckdb 加载向量数据库...\n```\n\n\n:::\n\n```{.python .cell-code}\ntry:\n    vector_store = DuckDBVectorStore(database_name=\"openrouter.duckdb\", persist_dir=\"./persist/\", read_only=True)\n    index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\nexcept Exception as e:\n    print(f\"⚠️ 无法加载向量数据库 (可能被锁定): {e}\")\n    # 创建一个空索引作为备选，或者跳过\n    index = None\n\n# 定义查询\nquery = \"What are model variants?\"\nprint(f\"\\n{'='*60}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n============================================================\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(f\"查询问题: '{query}'\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n查询问题: 'What are model variants?'\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(f\"{'='*60}\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n============================================================\n```\n\n\n:::\n\n```{.python .cell-code}\n# 检索前 3 个相关块\nif index:\n    retriever = index.as_retriever(similarity_top_k=5)\n    nodes = retriever.retrieve(query)\nelse:\n    nodes = []\n\n# 打印详细检索信息\nprint(f\"从 DuckDB 中检索到 {len(nodes)} 个文本块：\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n从 DuckDB 中检索到 5 个文本块：\n```\n\n\n:::\n\n```{.python .cell-code}\nfor i, node in enumerate(nodes, 1):\n    print(f\"{'─'*60}\")\n    print(f\"块 {i}\")\n    print(f\"{'─'*60}\")\n\n    # 打印相似度分数\n    if hasattr(node, 'score'):\n        print(f\"相似度分数: {node.score:.4f}\")\n\n    # 打印元数据\n    if hasattr(node, 'metadata') and node.metadata:\n        print(f\"元数据:\")\n        for key, value in node.metadata.items():\n            print(f\"  - {key}: {value}\")\n\n    # 打印文本内容（截断显示）\n    text_preview = node.text[:500] + \"...\" if len(node.text) > 500 else node.text\n    print(f\"\\n内容预览:\\n{text_preview}\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n````\n────────────────────────────────────────────────────────────\n块 1\n────────────────────────────────────────────────────────────\n相似度分数: 0.6170\n元数据:\n  - file_path: /Users/jinchaoduan/Documents/post_project/AI_Blog/posts/RAG/markdown_docs/docs_features_exacto-variant.md\n  - file_name: docs_features_exacto-variant.md\n  - file_type: text/markdown\n  - file_size: 7972\n  - creation_date: 2025-11-21\n  - last_modified_date: 2025-11-21\n\n内容预览:\nSearch\n\n`/`\n\nAsk AI\n\n[API](/docs/api-reference/overview)[Models](https://openrouter.ai/models)[Chat](https://openrouter.ai/chat)[Ranking](https://openrouter.ai/rankings)\n\n* Overview\n\n  + [Quickstart](/docs/quickstart)\n  + [FAQ](/docs/faq)\n  + [Principles](/docs/overview/principles)\n  + [Models](/docs/overview/models)\n  + [Enterprise](https://openrouter.ai/enterprise)\n* Features\n\n  + [Privacy and Logging](/docs/features/privacy-and-logging)\n  + [Zero Data Retention (ZDR)](/docs/features/zdr)\n  + ...\n\n────────────────────────────────────────────────────────────\n块 2\n────────────────────────────────────────────────────────────\n相似度分数: 0.6101\n元数据:\n  - file_path: /Users/jinchaoduan/Documents/post_project/AI_Blog/posts/RAG/markdown_docs/docs_overview_models.md\n  - file_name: docs_overview_models.md\n  - file_type: text/markdown\n  - file_size: 9021\n  - creation_date: 2025-11-21\n  - last_modified_date: 2025-11-21\n\n内容预览:\nSearch\n\n`/`\n\nAsk AI\n\n[API](/docs/api-reference/overview)[Models](https://openrouter.ai/models)[Chat](https://openrouter.ai/chat)[Ranking](https://openrouter.ai/rankings)\n\n* Overview\n\n  + [Quickstart](/docs/quickstart)\n  + [FAQ](/docs/faq)\n  + [Principles](/docs/overview/principles)\n  + [Models](/docs/overview/models)\n  + [Enterprise](https://openrouter.ai/enterprise)\n* Features\n\n  + [Privacy and Logging](/docs/features/privacy-and-logging)\n  + [Zero Data Retention (ZDR)](/docs/features/zdr)\n  + ...\n\n────────────────────────────────────────────────────────────\n块 3\n────────────────────────────────────────────────────────────\n相似度分数: 0.5821\n元数据:\n  - file_path: /Users/jinchaoduan/Documents/post_project/AI_Blog/posts/RAG/markdown_docs/docs_guides_overview_models.md\n  - file_name: docs_guides_overview_models.md\n  - file_type: text/markdown\n  - file_size: 7557\n  - creation_date: 2026-01-06\n  - last_modified_date: 2026-01-06\n\n内容预览:\nSearch\n\n`/`\n\nAsk AI\n\n[Models](https://openrouter.ai/models)[Chat](https://openrouter.ai/chat)[Ranking](https://openrouter.ai/rankings)[Docs](/docs/api-reference/overview)\n\n[Docs](/docs/quickstart)[API Reference](/docs/api/reference/overview)[SDK Reference](/docs/sdks/call-model/overview)\n\n[Docs](/docs/quickstart)[API Reference](/docs/api/reference/overview)[SDK Reference](/docs/sdks/call-model/overview)\n\n* Overview\n\n  + [Quickstart](/docs/quickstart)\n  + [Principles](/docs/guides/overview/princi...\n\n────────────────────────────────────────────────────────────\n块 4\n────────────────────────────────────────────────────────────\n相似度分数: 0.5763\n元数据:\n  - file_path: /Users/jinchaoduan/Documents/post_project/AI_Blog/posts/RAG/markdown_docs/docs_faq_how-are-rate-limits-calculated.md\n  - file_name: docs_faq_how-are-rate-limits-calculated.md\n  - file_type: text/markdown\n  - file_size: 17710\n  - creation_date: 2026-01-06\n  - last_modified_date: 2026-01-06\n\n内容预览:\nSearch\n\n`/`\n\nAsk AI\n\n[Models](https://openrouter.ai/models)[Chat](https://openrouter.ai/chat)[Ranking](https://openrouter.ai/rankings)[Docs](/docs/api-reference/overview)\n\n[Docs](/docs/quickstart)[API Reference](/docs/api/reference/overview)[SDK Reference](/docs/sdks/call-model/overview)\n\n[Docs](/docs/quickstart)[API Reference](/docs/api/reference/overview)[SDK Reference](/docs/sdks/call-model/overview)\n\n* Overview\n\n  + [Quickstart](/docs/quickstart)\n  + [Principles](/docs/guides/overview/princi...\n\n────────────────────────────────────────────────────────────\n块 5\n────────────────────────────────────────────────────────────\n相似度分数: 0.5703\n元数据:\n  - file_path: /Users/jinchaoduan/Documents/post_project/AI_Blog/posts/RAG/markdown_docs/docs_features_model-routing.md\n  - file_name: docs_features_model-routing.md\n  - file_type: text/markdown\n  - file_size: 7024\n  - creation_date: 2025-11-21\n  - last_modified_date: 2025-11-21\n\n内容预览:\n|\n| 17 | } |\n| 18 | ] |\n| 19 | ) |\n| 20 |  |\n| 21 | print(completion.choices[0].message.content) |\n```\n\nWas this page helpful?\n\nYesNo\n\n[Previous](/docs/features/zdr)[#### Provider Routing\n\nRoute requests to the best provider\n\nNext](/docs/features/provider-routing)[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=openrouter.ai)\n\n[![Logo](https://files.buildwithfern.com/openrouter.docs.buildwithfern.com/docs/2025-11-21T16:36:36.134Z/content/assets/logo.svg)!...\n````\n\n\n:::\n\n```{.python .cell-code}\n\n# Save retrieved chunks to a markdown file for easy inspection\n# with open(\"retriever.md\", \"w\", encoding=\"utf-8\") as f:\n#     f.write(f\"# Query: {query}\\n\\n\")\n#     f.write(f\"# Retrieved {len(nodes)} chunks from openrouter.duckdb\\n\\n\")\n#     for i, node in enumerate(nodes, 1):\n#         f.write(f\"{'─'*60}\\n\")\n#         f.write(f\"## Chunk {i}\\n\\n\")\n#         if hasattr(node, 'score'):\n#             f.write(f\"**Similarity Score:** {node.score:.4f}\\n\\n\")\n#         if hasattr(node, 'metadata') and node.metadata:\n#             f.write(f\"**Metadata:**\\n\")\n#             for key, value in node.metadata.items():\n#                 f.write(f\"- {key}: {value}\\n\")\n#             f.write(f\"\\n\")\n#         f.write(f\"{node.text}\\n\\n\")\n```\n:::\n\n\n\n\n\n## Python Chroma\n\n现在我们的知识库已经填充完毕，我们可以测试检索系统。我们可以提出一个特定的问题，例如“什么是模型变体？ (What are model variants?)”，并查询 `Chroma` 存储库以查看哪些文本块最相关。这确认了我们的嵌入和搜索是否正常工作。\n\n### 问题：什么是模型变体？ (What are model variants?)\n\nRAG 结果：\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom openai import OpenAI\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_chroma import Chroma\nfrom typing import List\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue\n```\n\n\n:::\n\n```{.python .cell-code}\n# 针对 OpenRouter API 的自定义嵌入类\nclass OpenRouterEmbeddings(Embeddings):\n    \"\"\"针对 OpenRouter API 的自定义嵌入类。\"\"\"\n    \n    def __init__(self, api_key: str, model: str = \"qwen/qwen3-embedding-8b\"):\n        self.client = OpenAI(\n            base_url=\"https://openrouter.ai/api/v1\",\n            api_key=api_key,\n        )\n        self.model = model\n    \n    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"对文档列表进行嵌入。\"\"\"\n        response = self.client.embeddings.create(\n            extra_headers={\n                \"HTTP-Referer\": \"https://ai-blog.com\",\n                \"X-Title\": \"AI Blog RAG\",\n            },\n            model=self.model,\n            input=texts,\n            encoding_format=\"float\"\n        )\n        return [item.embedding for item in response.data]\n    \n    def embed_query(self, text: str) -> List[float]:\n        \"\"\"对单个查询进行嵌入。\"\"\"\n        response = self.client.embeddings.create(\n            extra_headers={\n                \"HTTP-Referer\": \"https://ai-blog.com\",\n                \"X-Title\": \"AI Blog RAG\",\n            },\n            model=self.model,\n            input=text,\n            encoding_format=\"float\"\n        )\n        return response.data[0].embedding\n\n# 获取 OpenRouter API 密钥\nopenrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\nif not openrouter_api_key:\n    raise ValueError(\"未在环境变量中找到 OPENROUTER_API_KEY\")\n\n# 使用 OpenRouter 创建嵌入实例\nembeddings = OpenRouterEmbeddings(\n    api_key=openrouter_api_key,\n    model=\"qwen/qwen3-embedding-8b\"\n)\n\n# 定义向量数据库位置\npersist_directory = \"chroma_db_data\"\n\n# 加载现有的向量数据库\nvectorstore = Chroma(\n    persist_directory=persist_directory,\n    embedding_function=embeddings\n)\n\n# 测试查询\nquery = \"What are model variants?\"\n\n# 执行相似度搜索\nresults = vectorstore.similarity_search(query, k=5)\n\nprint(f\"\\n查询问题: '{query}'\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n查询问题: 'What are model variants?'\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(f\"找到 {len(results)} 个相关块：\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n找到 5 个相关块：\n```\n\n\n:::\n\n```{.python .cell-code}\nfor i, doc in enumerate(results, 1):\n    print(f\"结果 {i}:\")\n    print(f\"来源: {doc.metadata.get('source', '未知')}\")\n    print(f\"内容预览: {doc.page_content[:800]}...\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n结果 1:\n来源: docs_faq_how-are-rate-limits-calculated.md\n内容预览: ###### What are model variants?\n\nVariants are suffixes that can be added to the model slug to change its behavior.\n\nStatic variants can only be used with specific models and these are listed in our [models api](https://openrouter.ai/api/v1/models).\n\n1. `:free` - The model is always provided for free and has low rate limits.\n2. `:beta` - The model is not moderated by OpenRouter.\n3. `:extended` - The model has longer than usual context length.\n4. `:exacto` - The model only uses OpenRouter-curated high-quality endpoints.\n5. `:thinking` - The model supports reasoning by default.\n\nDynamic variants can be used on all models and they change the behavior of how the request is routed or used.\n\n1. `:online` - All requests will run a query to extract web results that are attached to the prompt.\n2. `:...\n结果 2:\n来源: docs_faq.md\n内容预览: ###### What are model variants?\n\nVariants are suffixes that can be added to the model slug to change its behavior.\n\nStatic variants can only be used with specific models and these are listed in our [models api](https://openrouter.ai/api/v1/models).\n\n1. `:free` - The model is always provided for free and has low rate limits.\n2. `:beta` - The model is not moderated by OpenRouter.\n3. `:extended` - The model has longer than usual context length.\n4. `:exacto` - The model only uses OpenRouter-curated high-quality endpoints.\n5. `:thinking` - The model supports reasoning by default.\n\nDynamic variants can be used on all models and they change the behavior of how the request is routed or used.\n\n1. `:online` - All requests will run a query to extract web results that are attached to the prompt.\n2. `:...\n结果 3:\n来源: docs_use-cases_crypto-api.md\n内容预览: [API](/docs/api-reference/overview)[Models](https://openrouter.ai/models)[Chat](https://openrouter.ai/chat)[Ranking](https://openrouter.ai/rankings)...\n结果 4:\n来源: docs_sdks_typescript.md\n内容预览: [API](/docs/api-reference/overview)[Models](https://openrouter.ai/models)[Chat](https://openrouter.ai/chat)[Ranking](https://openrouter.ai/rankings)...\n结果 5:\n来源: docs_features_provider-routing.md\n内容预览: Route requests through OpenRouter-curated providers\n\nNext](/docs/features/exacto-variant)[Built with](https://buildwithfern.com/?utm_campaign=buildWith&utm_medium=docs&utm_source=openrouter.ai)\n\n[![Logo](https://files.buildwithfern.com/openrouter.docs.buildwithfern.com/docs/2025-11-21T16:36:36.134Z/content/assets/logo.svg)![Logo](https://files.buildwithfern.com/openrouter.docs.buildwithfern.com/docs/2025-11-21T16:36:36.134Z/content/assets/logo-white.svg)](https://openrouter.ai/)\n\n[API](/docs/api-reference/overview)[Models](https://openrouter.ai/models)[Chat](https://openrouter.ai/chat)[Ranking](https://openrouter.ai/rankings)...\n```\n\n\n:::\n:::\n\n\n:::\n\n\n\n# 结合 RAG 进行聊天 (Chat with RAG)\n\n::: {.panel-tabset}\n\n## R\n\n最后一个环节是将这种检索能力连接到聊天界面。我们使用 `ellmer` 创建一个聊天客户端。关键在于，我们使用 `ragnar_register_tool_retrieve` 注册一个“检索工具”。这使 LLM 能够在需要信息回答用户问题时，自主查询我们的向量数据库。\n\n我们还提供了一个系统提示词，指示模型始终检查知识库并引用其来源。\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ellmer)\nlibrary(dotenv)\nlibrary(ragnar)\nload_dot_env(file = \".env\")\n\nchat <- chat_openrouter(\n    api_key = Sys.getenv(\"OPENROUTER_API_KEY\"),\n    model = \"openai/gpt-oss-120b\",\n    system_prompt = glue::trim(\"\n  你是一个负责问答任务的助手。请保持回复简练。\n\n  在回复之前，请从知识库中检索相关素材。引用或转述段落，清晰地标注哪些是你自己的话，哪些是来源。\n  为你引用的每个来源提供一个有效的链接，以及任何其他相关的链接。\n  除非你已经检索并引用了来源，否则不要回答。如果你没有找到相关信息，请说“我在知识库中找不到任何相关信息”。\n    \")\n)\n\n# 尝试连接存储并注册工具\nstore_connected <- FALSE\ntryCatch({\n    store <- ragnar_store_connect(\"openrouter.duckdb\")\n    chat <- chat |> ragnar_register_tool_retrieve(store, top_k = 3)\n    store_connected <- TRUE\n}, error = function(e) {\n    message(\"⚠️ 无法连接到 DuckDB 进行工具注册: \", e$message)\n})\n```\n:::\n\n\n### 问题：什么是模型变体？(What are model variants?)\n\n\n```{.r .cell-code}\nif (store_connected) {\n    chat$chat(\"What are model variants?\")\n    # 聊天结束后立即释放锁\n    rm(store)\n    gc()\n} else {\n    cat(\"由于数据库锁定，R 聊天功能暂时不可用。\")\n}\n```\n\n\n**模型变体（model variants）**是可以在模型标识（slug）后添加的后缀，用来改变模型的行为方式。  \n\n- **静态变体**只能用于特定模型，列在 [Models API](https://openrouter.ai/api/v1/models) 中。例如：  \n  1. `:free` – 始终免费提供，且速率限制较低。  \n  2. `:beta` – 不受 OpenRouter 内容审查。  \n  3. `:extended` – 提供比常规更长的上下文长度。  \n  4. `:exacto` – 仅使用 OpenRouter 精选的高质量端点。  \n  5. `:thinking` – 默认支持推理（reasoning）。\n\n- **动态变体**可以在所有模型上使用，改变请求的路由或使用方式。例如：  \n  1. `:online` – 在提示中附加网络搜索结果。  \n  2. `:nitro` – 按吞吐量排序提供者，优先更快响应。  \n  3. `:floor` – 按价格排序提供者，优先更具成本效益的选项。\n\n> “Variants are suffixes that can be added to the model slug to change its \nbehavior.”【来源: OpenRouter FAQ – 模型和提供者】(https://openrouter.ai/docs/faq)\n          used  (Mb) gc trigger  (Mb) limit (Mb) max used  (Mb)\nNcells 3083914 164.7    4675528 249.8         NA  4675528 249.8\nVcells 5315244  40.6   10146329  77.5      16384  9527284  72.7\n\n\n## Python chatlas\n\n我们还可以使用 `chatlas` 库来创建一个聊天界面。在这里，我们定义了一个自定义工具 `retrieve_trusted_content`，用于查询我们的 DuckDB 索引。然后我们将这个工具注册到聊天模型中，使其能够在回答用户问题时引入相关信息。\n\n### 问题：什么是模型变体？(What are model variants?)\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport os\nfrom typing import Any, List\nfrom openai import OpenAI\nimport chatlas as ctl\nfrom llama_index.core import VectorStoreIndex, Settings\nfrom llama_index.core.embeddings import BaseEmbedding\nfrom llama_index.vector_stores.duckdb import DuckDBVectorStore\nfrom dotenv import load_dotenv\nload_dotenv()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue\n```\n\n\n:::\n\n```{.python .cell-code}\n# 确保 API 密钥可用\nopenrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n\n# 针对 LlamaIndex 的自定义 OpenRouter 嵌入类\nclass OpenRouterEmbedding(BaseEmbedding):\n    \"\"\"与 LlamaIndex 兼容的 OpenRouter API 自定义嵌入类。\"\"\"\n\n    def __init__(\n        self,\n        api_key: str,\n        model: str = \"qwen/qwen3-embedding-8b\",\n        **kwargs: Any\n    ):\n        super().__init__(**kwargs)\n        self._client = OpenAI(\n            base_url=\"https://openrouter.ai/api/v1\",\n            api_key=api_key,\n        )\n        self._model = model\n\n    def _get_query_embedding(self, query: str) -> List[float]:\n        \"\"\"获取查询字符串的嵌入向量。\"\"\"\n        response = self._client.embeddings.create(\n            extra_headers={\n                \"HTTP-Referer\": \"https://ai-blog.com\",\n                \"X-Title\": \"AI Blog RAG\",\n            },\n            model=self._model,\n            input=query,\n            encoding_format=\"float\"\n        )\n        return response.data[0].embedding\n\n    def _get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"获取文本字符串的嵌入向量。\"\"\"\n        return self._get_query_embedding(text)\n\n    async def _aget_query_embedding(self, query: str) -> List[float]:\n        \"\"\"异步版本的 get_query_embedding。\"\"\"\n        return self._get_query_embedding(query)\n\n    async def _aget_text_embedding(self, text: str) -> List[float]:\n        \"\"\"异步版本的 get_text_embedding。\"\"\"\n        return self._get_text_embedding(text)\n\n# 1. 配置嵌入模型\nembed_model = OpenRouterEmbedding(\n    api_key=openrouter_api_key,\n    model=\"qwen/qwen3-embedding-8b\"\n)\nSettings.embed_model = embed_model\n\n# 2. 加载索引\ntry:\n    vector_store = DuckDBVectorStore(database_name=\"openrouter.duckdb\", persist_dir=\"./persist/\", read_only=True)\n    index = VectorStoreIndex.from_vector_store(vector_store=vector_store)\n    retriever = index.as_retriever(similarity_top_k=3)\nexcept Exception as e:\n    print(f\"⚠️ 无法加载向量数据库: {e}\")\n    index = None\n    retriever = None\n\n# 3. 定义聊天工具\ndef retrieve_trusted_content(question: str) -> str:\n    \"\"\"\n    在知识库中检索信任的内容来回答问题。\n    \"\"\"\n    if not retriever:\n        return \"知识库当前不可用（由于数据库锁定）。\"\n    nodes = retriever.retrieve(question)\n    combined_text = \"\\n\\n\".join([f\"Source Content {i+1}:\\n{node.text}\" for i, node in enumerate(nodes)])\n    return combined_text\n\n# 4. 设置聊天模型并注册工具\nchat = ctl.ChatOpenRouter(\n    model=\"openai/gpt-oss-120b\",\n    api_key=openrouter_api_key,\n    base_url=\"https://openrouter.ai/api/v1\",\n    system_prompt=\"\"\"\n    你是一个负责问答任务助手。请保持回复简练。\n    在回复之前，始终通过 retrieve_trusted_content 工具检索相关素材。\n    \"\"\"\n)\n\nchat.register_tool(retrieve_trusted_content)\n\n# 5. 执行聊天\ntry:\n    response = chat.chat(\"What are model variants?\")\n    print(response)\nexcept Exception as e:\n    print(f\"⚠️ 聊天过程出错 (可能是显示处理问题): {e}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<IPython.core.display.HTML object>\n<IPython.core.display.Markdown object>\n⚠️ 聊天过程出错 (可能是显示处理问题): Failed to create display handle\n```\n\n\n:::\n:::\n\n\n\n## Python LangChain\n\n在 Python 的 LangChain 中，我们设置了一个完整的 RAG 流水线。这包括：\n1. **检索器 (Retriever)**：使用我们的 `Chroma` 向量数据库。\n2. **提示词模版 (Prompt Template)**：指示模型使用检索到的上下文来回答问题。\n3. **聊天模型 (Chat Model)**：使用 OpenRouter 提供的 `gpt-4o`。\n4. **输出解析器 (Output Parser)**：用于格式化最终响应。\n\n这种模块化方法是构建生产级 AI 应用程序的典型方式。\n\n### 问题：什么是模型变体？(What are model variants?)\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\nfrom openai import OpenAI\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_chroma import Chroma\nfrom typing import List\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTrue\n```\n\n\n:::\n\n```{.python .cell-code}\n# 针对 LangChain 的自定义 OpenRouter 嵌入类\nclass OpenRouterEmbeddings(Embeddings):\n    def __init__(self, api_key: str, model: str = \"qwen/qwen3-embedding-8b\"):\n        self._client = OpenAI(\n            base_url=\"https://openrouter.ai/api/v1\",\n            api_key=api_key,\n        )\n        self._model = model\n\n    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n        \"\"\"嵌入文档列表。\"\"\"\n        response = self._client.embeddings.create(\n            model=self._model,\n            input=texts\n        )\n        return [d.embedding for d in response.data]\n\n    def embed_query(self, text: str) -> List[float]:\n        \"\"\"嵌入单个查询。\"\"\"\n        response = self._client.embeddings.create(\n            model=self._model,\n            input=text\n        )\n        return response.data[0].embedding\n\n# 获取 OpenRouter API 密钥\nopenrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\nif not openrouter_api_key:\n    raise ValueError(\"未在环境变量中找到 OPENROUTER_API_KEY\")\n\n# 使用 OpenRouter 创建嵌入实例\nembeddings = OpenRouterEmbeddings(\n    api_key=openrouter_api_key,\n    model=\"qwen/qwen3-embedding-8b\"\n)\n\n# 定义向量数据库位置\npersist_directory = \"chroma_db_data\"\n\n# 加载现有的向量数据库\nprint(f\"正在从 {persist_directory} 加载现有向量数据库...\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n正在从 chroma_db_data 加载现有向量数据库...\n```\n\n\n:::\n\n```{.python .cell-code}\nvectorstore = Chroma(\n    persist_directory=persist_directory,\n    embedding_function=embeddings\n)\nprint(f\"✓ 向量数据库加载成功\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n✓ 向量数据库加载成功\n```\n\n\n:::\n\n```{.python .cell-code}\n# 使用 OpenRouter 初始化 LLM\nllm = ChatOpenAI(\n    model=\"openai/gpt-oss-120b\",\n    openai_api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n    openai_api_base=\"https://openrouter.ai/api/v1\"\n)\n\n# 创建提示词模版\nsystem_prompt = (\n    \"你是一个负责问答任务助手。\"\n    \"请使用以下检索到的素材来回答问题。\"\n    \"如果你不知道答案，请不要胡编乱造。\"\n    \"最多使用三句话，并保持回复简练。\"\n    \"\\n\\n\"\n    \"素材: {context}\"\n    \"\\n\\n\"\n    \"问题: {question}\"\n)\n\nprompt = ChatPromptTemplate.from_template(system_prompt)\n\n# 创建检索器\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n\n# 用于格式化文档的辅助函数\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n# 使用 LCEL 构建 RAG 链\nrag_chain = (\n    {\n        \"context\": retriever | format_docs,\n        \"question\": RunnablePassthrough()\n    }\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n\nprint(\"✓ RAG 链创建成功！\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n✓ RAG 链创建成功！\n```\n\n\n:::\n\n```{.python .cell-code}\n# 测试 RAG 链\nquestion = \"What are model variants?\"\n\nprint(f\"\\n查询问题: {question}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n查询问题: What are model variants?\n```\n\n\n:::\n\n```{.python .cell-code}\n# 独立获取上下文文档以便展示\ncontext_docs = retriever.invoke(question)\n\n# 调用 RAG 链\nanswer = rag_chain.invoke(question)\nimport textwrap\n\nfor line in answer.split('\\n'):\n    print(textwrap.fill(line, width=80))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n模型变体是可以附加在模型标识符后面的后缀，用来改变化模型的行为或路由方式。静态变体（如\n`:free`、`:beta`、`:extended`、`:exacto`、`:thinking`）仅适用于特定模型，而动态变体（如\n`:online`、`:nitro`、`:floor`）可在所有模型上使用，分别提供网络搜索、优先高吞吐或低价格的路由策略。\n```\n\n\n:::\n:::\n\n\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}